{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hRYJEm6w8HlO",
    "outputId": "b49989f0-3431-48b8-9da1-f384d63eff7b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported.\n"
     ]
    }
   ],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# ║ 0.C SET-UP: Imports                                                   ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "import random\n",
    "import math\n",
    "import time\n",
    "import json\n",
    "import shutil\n",
    "import warnings\n",
    "import itertools\n",
    "from pathlib import Path # Using pathlib can be nice for paths\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch_geometric.data import Data, Dataset, Batch\n",
    "from torch_geometric.loader import DataLoader # Corrected import for DataLoader\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv, global_mean_pool\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "import wandb\n",
    "\n",
    "PROJECT_BASE_DIR = \"../EEG_nml\" # ✏️ ADJUST THIS PATH!\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "print(\"Libraries imported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 338
    },
    "id": "xhvjugFI8JMl",
    "outputId": "a0946c8f-d838-4d0b-e1d0-7ba58b15f7fd"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/my_notebooks/EEG_nml/wandb/run-20250528_120258-x04vg56z</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/x04vg56z' target=\"_blank\">still-dream-40</a></strong> to <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/x04vg56z' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/x04vg56z</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB initialized. Project: eeg-gnn-group-project, Entity: danielebelfiore7-epfl\n",
      "Run link: https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/x04vg56z\n"
     ]
    }
   ],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# ║ 1. CONFIGURATION                                                      ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "config = dict(\n",
    "    project         = \"eeg-gnn-group-project\",  # ✏️ ADJUST WandB project name if needed\n",
    "    entity          = \"danielebelfiore7-epfl\", # ✏️ ADJUST your WandB entity (username or team)\n",
    "    gnn_type        = \"sage\",                    # \"gcn\" | \"sage\" | \"gat\"\n",
    "    edge_variant    = \"knn\",                   # \"grid\" | \"knn\" (ensure corresponding .pt file exists)\n",
    "    hidden_dim      = 64,\n",
    "    num_layers      = 6,\n",
    "    dropout         = 0.25,\n",
    "    lr              = 5e-5,\n",
    "    weight_decay    = 1e-4,\n",
    "    batch_size      = 32,\n",
    "    epochs          = 25,                       # Can reduce for quick testing (e.g., 5)\n",
    "    seed            = 1,\n",
    "    base_dir        = PROJECT_BASE_DIR          # Using the verified path from Cell 1\n",
    ")\n",
    "\n",
    "# Initialize Weights & Biases (do this once at the beginning)\n",
    "try:\n",
    "    wandb.init(project=config[\"project\"], entity=config[\"entity\"],\n",
    "               config=config, mode=\"online\") # Use \"disabled\" for no logging during tests\n",
    "    print(f\"WandB initialized. Project: {config['project']}, Entity: {config['entity']}\")\n",
    "    print(f\"Run link: {wandb.run.get_url() if wandb.run else 'WandB run not active (possibly disabled mode)'}\")\n",
    "except Exception as e:\n",
    "    print(f\"WandB initialization failed: {e}\")\n",
    "    print(\"Continuing without WandB logging. Check your entity and project name.\")\n",
    "    # Fallback to disabled mode if init fails\n",
    "    if wandb.run: wandb.finish(quiet=True) # Ensure any partial run is closed\n",
    "    wandb.init(project=config[\"project\"], mode=\"disabled\", config=config)\n",
    "    print(\"WandB running in disabled mode.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "9QzznyNl8Kaa",
    "outputId": "34175657-a103-4092-80c9-5da0ab64da25"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Seed set to: 1\n"
     ]
    }
   ],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# ║ 2. REPRODUCIBILITY                                                    ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed) # for multi-GPU.\n",
    "    # Potentially add:\n",
    "    # torch.backends.cudnn.deterministic = True\n",
    "    # torch.backends.cudnn.benchmark = False\n",
    "\n",
    "set_seed(config[\"seed\"])\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "print(f\"Seed set to: {config['seed']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gqZnYFbx0pOc",
    "outputId": "7d59cedb-3d75-4505-ccf0-ac5b75da7abf"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Inspecting TRAIN Metadata (../EEG_nml/train_data/metadata.parquet) ---\n",
      "Successfully loaded ../EEG_nml/train_data/metadata.parquet\n",
      "\n",
      "Total rows in metadata: 12993\n",
      "\n",
      "Columns in metadata.parquet:\n",
      "['patient', 'session', 'segment', 'label', 'start_time', 'end_time', 'date', 'sampling_rate', 'signals_path']\n",
      "\n",
      "Metadata Index Type: <class 'pandas.core.indexes.range.RangeIndex'>\n",
      "Metadata Index Names: [None]\n",
      "First 3 index entries: [0, 1, 2]\n",
      "Are indices unique? True\n",
      "\n",
      "First 3 rows of metadata.parquet:\n",
      "    patient    session  segment  label  start_time  end_time       date  \\\n",
      "0  pqejgcff  s001_t000        0      1         0.0      12.0 2003-01-01   \n",
      "1  pqejgcff  s001_t000        1      1        12.0      24.0 2003-01-01   \n",
      "2  pqejgcff  s001_t000        2      1        24.0      36.0 2003-01-01   \n",
      "\n",
      "   sampling_rate                        signals_path  \n",
      "0            250  signals/pqejgcff_s001_t000.parquet  \n",
      "1            250  signals/pqejgcff_s001_t000.parquet  \n",
      "2            250  signals/pqejgcff_s001_t000.parquet  \n",
      "\n",
      "Missing values in 'signals_path': 0\n",
      "\n",
      "Missing values in 'segment': 0\n",
      "\n",
      "Missing values in 'label': 0\n",
      "\n",
      "Data around the 999th entry (0-indexed, so rows 998, 999, 1000):\n",
      "       patient    session  segment  label  start_time  end_time       date  \\\n",
      "998   pqejgect  s001_t000       37      1       444.0     456.0 2004-01-01   \n",
      "999   pqejgect  s001_t000       38      1       456.0     468.0 2004-01-01   \n",
      "1000  pqejgect  s001_t000       39      0       468.0     480.0 2004-01-01   \n",
      "\n",
      "      sampling_rate                        signals_path  \n",
      "998             250  signals/pqejgect_s001_t000.parquet  \n",
      "999             250  signals/pqejgect_s001_t000.parquet  \n",
      "1000            250  signals/pqejgect_s001_t000.parquet  \n"
     ]
    }
   ],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# ║ 3a. INSPECT TRAIN DATA                                               ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np # For checking .npy files later\n",
    "\n",
    "# Ensure config is defined if you're running this in your main notebook\n",
    "# If not, define PROJECT_BASE_DIR directly:\n",
    "# PROJECT_BASE_DIR = \"/content/drive/MyDrive/EEG_nml\"\n",
    "PROJECT_BASE_DIR = config[\"base_dir\"] # Assuming 'config' is available from your notebook\n",
    "\n",
    "train_meta_path = os.path.join(PROJECT_BASE_DIR, \"train_data\", \"metadata.parquet\")\n",
    "\n",
    "print(f\"--- Inspecting TRAIN Metadata ({train_meta_path}) ---\")\n",
    "if os.path.exists(train_meta_path):\n",
    "    try:\n",
    "        df_meta = pd.read_parquet(train_meta_path)\n",
    "        print(f\"Successfully loaded {train_meta_path}\")\n",
    "        print(f\"\\nTotal rows in metadata: {len(df_meta)}\")\n",
    "        print(f\"\\nColumns in metadata.parquet:\")\n",
    "        print(list(df_meta.columns))\n",
    "\n",
    "        print(f\"\\nMetadata Index Type: {type(df_meta.index)}\")\n",
    "        print(f\"Metadata Index Names: {df_meta.index.names}\")\n",
    "        if not df_meta.empty:\n",
    "            print(f\"First 3 index entries: {list(df_meta.index[:3])}\")\n",
    "            print(f\"Are indices unique? {df_meta.index.is_unique}\")\n",
    "\n",
    "        print(f\"\\nFirst 3 rows of metadata.parquet:\")\n",
    "        print(df_meta.head(3))\n",
    "\n",
    "        # Check for NaN/missing values in critical columns\n",
    "        critical_cols = [\"signals_path\", \"segment\", \"label\"] # Based on your script\n",
    "        for col in critical_cols:\n",
    "            if col in df_meta.columns:\n",
    "                print(f\"\\nMissing values in '{col}': {df_meta[col].isnull().sum()}\")\n",
    "            else:\n",
    "                print(f\"\\nColumn '{col}' NOT FOUND in metadata!\")\n",
    "\n",
    "        # Try to identify what data might be around the 999th entry (0-indexed)\n",
    "        if len(df_meta) > 999:\n",
    "            print(\"\\nData around the 999th entry (0-indexed, so rows 998, 999, 1000):\")\n",
    "            print(df_meta.iloc[998:1001])\n",
    "        elif not df_meta.empty:\n",
    "            print(\"\\nMetadata has fewer than 1000 entries. Showing last few entries:\")\n",
    "            print(df_meta.tail(3))\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading or inspecting {train_meta_path}: {e}\")\n",
    "else:\n",
    "    print(f\"File NOT FOUND: {train_meta_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 988,
     "referenced_widgets": [
      "68983a09643c415cb8fd351faef97383",
      "54d6eeecf4d04cbe88d10de85ca1445a",
      "ced5182567074f96b00111c4c9a18985",
      "493d65c0b607400f8a95cab5a903dc7d",
      "a56514f486f64509b8ebf56d57ad31bb",
      "9fe842fcd2fb4ed4a174ac383ea747db",
      "84fc0f3ec9954ab8a001f0f843ccc5a8",
      "6ed01728c0f340bfab91446c373f7774",
      "7da68b8fa80a470eb3f5f9d61b9bafc6",
      "cb23647d373a41ada5a4d68c6e027a98",
      "e093e37434bf4fb38f8c7fdb319eaaf0"
     ]
    },
    "id": "FrIoi0rW8L19",
    "outputId": "aa75eb77-8c93-46d1-e196-e9cac46df271"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Old caching folder /home/.cache/huggingface/datasets/eeg_nml/default-data_dir=..%2FEEG_nml/1.0.0/16d9681eeba9a735676e78110d14b899d1de320df916d70e9f8d11ee07540654 for dataset eeg_nml exists but no data were found. Removing it. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted hf_train\n",
      "Garbage collection run.\n",
      "Attempting to load dataset using script: ../EEG_nml/EEG_nml.py\n",
      "Using data_dir for script: ../EEG_nml\n",
      "../EEG_nml\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d16413ddef4d441e95c8139145d608a6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cbcdc15418af459c8d0961283c9608f2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating test split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HF train dataset loaded successfully. Size: 12993\n",
      "Train/validation split created: 10394 train, 2599 validation samples.\n"
     ]
    }
   ],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# ║ 3b. LOAD HUGGING-FACE DATASET (TRAIN SPLIT)                          ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "import gc\n",
    "\n",
    "# List all potential variables that might hold references\n",
    "vars_to_delete = ['hf_train', 'hf_test', 'train_ds', 'val_ds', 'test_ds',\n",
    "                  'train_loader', 'val_loader', 'test_loader', 'model']\n",
    "\n",
    "for var_name in vars_to_delete:\n",
    "    if var_name in globals():\n",
    "        try:\n",
    "            del globals()[var_name]\n",
    "            print(f\"Deleted {var_name}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Could not delete {var_name}: {e}\")\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "print(\"Garbage collection run.\")\n",
    "\n",
    "\n",
    "SCRIPT_PATH = os.path.join(config[\"base_dir\"], \"EEG_nml.py\")\n",
    "print(f\"Attempting to load dataset using script: {SCRIPT_PATH}\")\n",
    "print(f\"Using data_dir for script: {config['base_dir']}\")\n",
    "\n",
    "hf_train = None # Initialize to allow later check\n",
    "\n",
    "hf_train = load_dataset(path=SCRIPT_PATH, data_dir=config[\"base_dir\"], split=\"train\", name=\"default\", download_mode=\"force_redownload\")\n",
    "\n",
    "try:\n",
    "    # Assumes EEG_nml.py uses config[\"base_dir\"] to find \"train_data\" for split=\"train\"\n",
    "    print(f\"HF train dataset loaded successfully. Size: {len(hf_train)}\")\n",
    "\n",
    "    # Simple 80/20 random split for validation from the training patient data\n",
    "    # This creates a validation set from the 50 training patients\n",
    "    if len(hf_train) > 0:\n",
    "        split_idx = int(0.8 * len(hf_train))\n",
    "        perm      = np.random.permutation(len(hf_train))\n",
    "        train_idx, val_idx = perm[:split_idx], perm[split_idx:]\n",
    "        print(f\"Train/validation split created: {len(train_idx)} train, {len(val_idx)} validation samples.\")\n",
    "    else:\n",
    "        print(\"Warning: Loaded training dataset is empty. Cannot create train/val split.\")\n",
    "        train_idx, val_idx = [], []\n",
    "\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print(f\"ERROR loading training dataset: {e}\")\n",
    "    print(f\"Please check SCRIPT_PATH ('{SCRIPT_PATH}') and that EEG_nml.py is correctly configured.\")\n",
    "    print(f\"Ensure 'EEG_nml.py' can find the 'train_data' folder within '{config['base_dir']}' when split='train' is used.\")\n",
    "    print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    # Optional: Stop execution\n",
    "    # raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label distribution in hf_train:\n",
      "Counter({0: 10476, 1: 2517})\n",
      "Class 0 percentage: 80.63%\n",
      "Class 1 percentage: 19.37%\n",
      "tensor([4.1621], device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "# In a new cell after loading hf_train or from df_meta in Cell 4\n",
    "# Assuming 'label' is the column name\n",
    "# If using df_meta from Cell 4:\n",
    "# label_counts = df_meta['label'].value_counts()\n",
    "# print(f\"Label distribution in metadata:\\n{label_counts}\")\n",
    "\n",
    "# If using hf_train from Cell 5:\n",
    "# Assuming hf_train is a Hugging Face Dataset object\n",
    "labels = [sample['label'] for sample in hf_train]\n",
    "import collections\n",
    "label_counts = collections.Counter(labels)\n",
    "print(f\"Label distribution in hf_train:\\n{label_counts}\")\n",
    "print(f\"Class 0 percentage: {label_counts[0] / len(labels) * 100:.2f}%\")\n",
    "print(f\"Class 1 percentage: {label_counts[1] / len(labels) * 100:.2f}%\")\n",
    "pos_weight_tensor_for_train = pos_w = torch.tensor([(label_counts[0] / len(labels))/(label_counts[1] / len(labels))], device=device)\n",
    "print(pos_weight_tensor_for_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CFvwODq38NvI",
    "outputId": "0cad3493-12c7-4ef1-f931-5ea86f201c09"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to load edge index from: ../EEG_nml/edge_index/edge_index_knn.pt\n",
      "Edge index loaded successfully. Shape: torch.Size([2, 152])\n",
      "GraphEEGDataset class defined.\n"
     ]
    }
   ],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# ║ 4.A DEFINE PYTORCH GEOMETRIC DATASET CLASS & LOAD EDGE INDEX         ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "EDGE_INDEX = None # Initialize\n",
    "edge_path = os.path.join(config[\"base_dir\"], \"edge_index\", f\"edge_index_{config['edge_variant']}.pt\")\n",
    "print(f\"Attempting to load edge index from: {edge_path}\")\n",
    "\n",
    "try:\n",
    "    EDGE_INDEX = torch.load(edge_path).long()  # shape (2, E)\n",
    "    print(f\"Edge index loaded successfully. Shape: {EDGE_INDEX.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print(f\"ERROR: Edge index file NOT FOUND at {edge_path}\")\n",
    "    print(f\"Please ensure the 'edge_index' folder and the file 'edge_index_{config['edge_variant']}.pt' exist in '{os.path.join(config['base_dir'], 'edge_index')}'.\")\n",
    "    print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    # Optional: Stop execution\n",
    "    # raise\n",
    "except Exception as e:\n",
    "    print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print(f\"An unexpected error occurred while loading edge index: {e}\")\n",
    "    print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    # Optional: Stop execution\n",
    "    # raise\n",
    "\n",
    "class GraphEEGDataset(Dataset):\n",
    "    def __init__(self, hf_dataset, indices):\n",
    "        super().__init__()\n",
    "        if hf_dataset is None or len(indices) == 0 and len(hf_dataset)>0 : # check if hf_dataset is None or if indices are empty for non-empty dataset\n",
    "             print(\"Warning: GraphEEGDataset initialized with None or empty hf_dataset/indices.\")\n",
    "             self.hf = [] # Make it an empty list to avoid errors in len/get\n",
    "        elif len(hf_dataset) == 0 and len(indices) > 0 :\n",
    "             print(\"Warning: GraphEEGDataset initialized with empty hf_dataset but non-empty indices.\")\n",
    "             self.hf = []\n",
    "        else:\n",
    "             self.hf = hf_dataset.select(indices)\n",
    "\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.hf)\n",
    "\n",
    "    def get(self, idx):\n",
    "        if not self.hf: # Handle empty dataset case\n",
    "            raise IndexError(\"Attempting to get item from an empty or uninitialized hf_dataset in GraphEEGDataset\")\n",
    "        row = self.hf[idx]\n",
    "        x = torch.tensor(row[\"features\"], dtype=torch.float)  # (19,9) - Ensure dtype\n",
    "        y = torch.tensor([row[\"label\"]], dtype=torch.float) # shape (1,) - Ensure dtype\n",
    "\n",
    "        # --- Construct the new composite signal_id ---\n",
    "        base_id = row[\"signal_id\"]      # e.g., \"pqejgcpt_s001_t000\"\n",
    "        segment_num = row[\"segment\"]    # e.g., 0, 1, 2\n",
    "\n",
    "        # Create the composite ID by appending segment number to the base ID\n",
    "        # This matches your example: \"pqejgcpt_s001_t000\" + \"0\" -> \"pqejgcpt_s001_t0000\"\n",
    "        composite_id = f\"{base_id}_{segment_num}\"\n",
    "        \n",
    "        # Make sure EDGE_INDEX is loaded, otherwise this will fail\n",
    "        if EDGE_INDEX is None:\n",
    "            raise ValueError(\"EDGE_INDEX is not loaded. Cannot create Data object.\")\n",
    "        \n",
    "        # Optional: Add a debug print for the first few IDs to verify\n",
    "        if idx < 5 and self.hf.split == 'test': # Print only for test set and first 5 items\n",
    "             print(f\"DEBUG GraphEEGDataset.get() (Test Split): base_id='{base_id}', segment_num='{segment_num}', CREATED composite_id='{composite_id}'\")\n",
    "        \n",
    "        data = Data(x=x, edge_index=EDGE_INDEX, y=y,\n",
    "                    signal_id=composite_id) # Make sure 'signal_id' exists\n",
    "        return data\n",
    "\n",
    "print(\"GraphEEGDataset class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "PDMRcqXj8P_T",
    "outputId": "bd4ba0b5-ae36-491c-8ec7-77196ded41a5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyG Datasets and DataLoaders created.\n",
      "Loaded: 10394 train | 2599 validation examples.\n",
      "Sample data from train_ds[0]: Data(x=[19, 9], edge_index=[2, 152], y=[1], signal_id='pqejgetp_s011_t001_40')\n",
      "Node features shape: torch.Size([19, 9]), Label: tensor([0.])\n",
      "Sample data from val_ds[0]: Data(x=[19, 9], edge_index=[2, 152], y=[1], signal_id='pqejgnkx_s002_t006_26')\n"
     ]
    }
   ],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# ║ 4.B CREATE PYG DATASETS AND DATALOADERS (TRAIN & VALIDATION)         ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "if hf_train and EDGE_INDEX is not None and train_idx is not None and val_idx is not None:\n",
    "    train_ds = GraphEEGDataset(hf_train, train_idx)\n",
    "    val_ds   = GraphEEGDataset(hf_train, val_idx)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=config[\"batch_size\"], shuffle=True, drop_last=True if len(train_ds) > config[\"batch_size\"] else False)\n",
    "    val_loader   = DataLoader(val_ds, batch_size=config[\"batch_size\"], shuffle=False, drop_last=False)\n",
    "\n",
    "    print(f\"PyG Datasets and DataLoaders created.\")\n",
    "    print(f\"Loaded: {len(train_ds)} train | {len(val_ds)} validation examples.\")\n",
    "    if len(train_ds) > 0:\n",
    "      print(f\"Sample data from train_ds[0]: {train_ds[0]}\")\n",
    "      print(f\"Node features shape: {train_ds[0].x.shape}, Label: {train_ds[0].y}\")\n",
    "    if len(val_ds) > 0:\n",
    "      print(f\"Sample data from val_ds[0]: {val_ds[0]}\")\n",
    "else:\n",
    "    print(\"Skipping DataLoader creation due to previous errors (hf_train, EDGE_INDEX, or splits not available).\")\n",
    "    train_loader, val_loader = None, None # Ensure they are defined for later checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yloVmSN98Shu",
    "outputId": "5dfb8b81-0cd0-4487-d4f9-c43071df46e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEGGNN class defined.\n"
     ]
    }
   ],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# ║ 5. (OLD) MODULAR GNN CLASS                                           ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "CONV_MAP = dict(\n",
    "    gcn  = GCNConv,\n",
    "    sage = SAGEConv,\n",
    "    gat  = lambda in_c, out_c: GATConv(in_c, out_c, heads=4, concat=True) # Single head, no dim expansion\n",
    ")\n",
    "\n",
    "class EEGGNN(nn.Module):\n",
    "    def __init__(self, gnn_type, in_dim=9, hidden_dim=64, num_layers=2, dropout=0.3, num_classes=1):\n",
    "        super().__init__()\n",
    "        if gnn_type not in CONV_MAP:\n",
    "            raise ValueError(f\"Unsupported GNN type: {gnn_type}. Supported types are {list(CONV_MAP.keys())}\")\n",
    "\n",
    "        ConvLayer = CONV_MAP[gnn_type]\n",
    "        self.convs = nn.ModuleList()\n",
    "\n",
    "        # First layer\n",
    "        self.convs.append(ConvLayer(in_dim, hidden_dim))\n",
    "\n",
    "        # Hidden layers\n",
    "        for _ in range(num_layers - 1):\n",
    "            self.convs.append(ConvLayer(hidden_dim, hidden_dim))\n",
    "\n",
    "        self.dropout_rate = dropout\n",
    "        self.cls = nn.Linear(hidden_dim, num_classes) # num_classes usually 1 for binary_cross_entropy_with_logits\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        if x is None or edge_index is None:\n",
    "             raise ValueError(\"Input data (x or edge_index) is None in EEGGNN forward pass.\")\n",
    "\n",
    "        for i, conv in enumerate(self.convs):\n",
    "            x = conv(x, edge_index)\n",
    "            x = F.relu(x)\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        # Global pooling\n",
    "        if batch is None : # Handle cases where batch might be missing if a single graph is passed without a loader\n",
    "             if x.size(0) > 0 : # Check if x has any nodes\n",
    "                  g = global_mean_pool(x, torch.zeros(x.size(0), dtype=torch.long, device=x.device))\n",
    "             else: # Handle empty graph case after convolutions\n",
    "                  # Return a zero tensor of appropriate shape or handle error\n",
    "                  # For now, let's assume this case should not happen with proper data\n",
    "                  # Or, if it does, the classifier input needs to be handled.\n",
    "                  # This might indicate an issue upstream if graphs become empty.\n",
    "                  print(\"Warning: Graph has no nodes after convolutions before global_mean_pool.\")\n",
    "                  # Create a zero tensor for the classifier input, matching hidden_dim\n",
    "                  # This part might need adjustment based on how num_classes is handled\n",
    "                  # If num_classes is 1, then shape is (1, hidden_dim) for pool, then (1,1) for cls output\n",
    "                  g = torch.zeros((data.num_graphs if hasattr(data, 'num_graphs') and data.num_graphs > 0 else 1, self.cls.in_features), device=x.device if x.device else device)\n",
    "        else:\n",
    "             g = global_mean_pool(x, batch)  # (batch_size, hidden_dim)\n",
    "\n",
    "        out = self.cls(g) # (batch_size, num_classes)\n",
    "\n",
    "        # If num_classes is 1 (for BCEWithLogitsLoss), squeeze the last dimension\n",
    "        if self.cls.out_features == 1:\n",
    "            out = out.squeeze(1) # (batch_size,)\n",
    "\n",
    "        return out\n",
    "\n",
    "print(\"EEGGNN class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EEGGNN multi-head class defined.\n"
     ]
    }
   ],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# ║ 5. MULTI-HEAD-READY GNN CLASS                                        ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, GATConv, global_mean_pool, BatchNorm\n",
    "\n",
    "# Assume CONV_MAP and GAT_HEADS are defined as above\n",
    "\n",
    "class EEGGNN(nn.Module):\n",
    "    def __init__(self, gnn_type, in_dim=9, hidden_dim=64, num_layers=2, dropout=0.3, num_classes=1):\n",
    "        super().__init__()\n",
    "        if gnn_type not in CONV_MAP:\n",
    "            raise ValueError(f\"Unsupported GNN type: {gnn_type}. Supported types are {list(CONV_MAP.keys())}\")\n",
    "\n",
    "        ConvLayer = CONV_MAP[gnn_type]\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.bns = nn.ModuleList()\n",
    "        self.dropout_rate = dropout\n",
    "        self.num_layers = num_layers\n",
    "        self.gnn_type = gnn_type # Store gnn_type\n",
    "\n",
    "        current_dim = in_dim\n",
    "\n",
    "        for i in range(num_layers):\n",
    "            # For GAT with concat=True, the output dim of the conv layer is hidden_dim * heads\n",
    "            # The input to the *next* GAT layer's internal linear projection will handle this.\n",
    "            # The GATConv layer itself will output hidden_dim * heads if concat=True.\n",
    "            \n",
    "            output_channels_for_conv = hidden_dim # This is 'out_channels' per head for GAT\n",
    "\n",
    "            self.convs.append(ConvLayer(current_dim, output_channels_for_conv))\n",
    "            \n",
    "            # Determine the dimension after the convolution layer\n",
    "            if gnn_type == \"gat\" and GAT_HEADS > 1 and getattr(self.convs[-1], 'concat', False): # Check if concat is True\n",
    "                dim_after_conv = output_channels_for_conv * GAT_HEADS\n",
    "            else:\n",
    "                dim_after_conv = output_channels_for_conv\n",
    "            \n",
    "            self.bns.append(BatchNorm(dim_after_conv))\n",
    "            current_dim = dim_after_conv # Input dim for the next layer\n",
    "\n",
    "        # The final classifier's input dimension must match the output of the last GNN block\n",
    "        self.cls = nn.Linear(current_dim, num_classes)\n",
    "\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        if x is None or edge_index is None:\n",
    "             raise ValueError(\"Input data (x or edge_index) is None in EEGGNN forward pass.\")\n",
    "\n",
    "        for i in range(self.num_layers):\n",
    "            x_input_for_skip = x \n",
    "\n",
    "            x = self.convs[i](x, edge_index)\n",
    "            x = self.bns[i](x)\n",
    "            x = F.relu(x)\n",
    "            \n",
    "            # Skip connection: Ensure dimensions match or use a projection\n",
    "            # This basic skip assumes the output of ReLU (x) has the same dim as x_input_for_skip\n",
    "            # This might need adjustment if dimensions change significantly due to GAT concat.\n",
    "            # A common pattern for GAT with concat is that the *next* layer takes hidden_dim * heads as input.\n",
    "            # If you add skip connections, project x_input_for_skip to match x's dimension if they differ.\n",
    "            # For simplicity, let's assume skip connections are primarily for non-GAT or GAT with average\n",
    "            if self.gnn_type != \"gat\" or not getattr(self.convs[i], 'concat', False) or GAT_HEADS == 1:\n",
    "                 if i > 0 and x_input_for_skip.shape == x.shape:\n",
    "                     x = x + x_input_for_skip\n",
    "            # If GAT with concat, managing skip connections requires careful dimension handling\n",
    "            # or a projection layer for the skip path.\n",
    "\n",
    "            x = F.dropout(x, p=self.dropout_rate, training=self.training)\n",
    "\n",
    "        if batch is None:\n",
    "             g = global_mean_pool(x, torch.zeros(x.size(0), dtype=torch.long, device=x.device)) \\\n",
    "                 if x.size(0) > 0 else torch.zeros((1, self.cls.in_features), device=x.device if x.device else 'cpu')\n",
    "        else:\n",
    "             g = global_mean_pool(x, batch)\n",
    "\n",
    "        out = self.cls(g)\n",
    "\n",
    "        if self.cls.out_features == 1:\n",
    "            out = out.squeeze(1)\n",
    "        return out\n",
    "print(\"EEGGNN multi-head class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7BJRKkW48UyQ",
    "outputId": "7d8b53b7-21e0-49ee-9b2a-019e14d3aa0d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input dimension automatically detected from data: 9\n",
      "Model and Optimizer instantiated.\n",
      "EEGGNN(\n",
      "  (convs): ModuleList(\n",
      "    (0): SAGEConv(9, 64, aggr=mean)\n",
      "    (1-5): 5 x SAGEConv(64, 64, aggr=mean)\n",
      "  )\n",
      "  (cls): Linear(in_features=64, out_features=1, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# ║ 5.B INSTANTIATE MODEL & OPTIMIZER                                     ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "# Determine input dimension from data if possible, otherwise default to 9\n",
    "# This assumes train_ds has been successfully created.\n",
    "# Sample one data point to get in_dim (node feature dimension)\n",
    "in_dim_data = 9 # Default\n",
    "if train_loader and len(train_ds) > 0:\n",
    "    try:\n",
    "        sample_data_for_dim = train_ds.get(0)\n",
    "        if hasattr(sample_data_for_dim, 'x') and sample_data_for_dim.x is not None:\n",
    "            in_dim_data = sample_data_for_dim.x.shape[1]\n",
    "        print(f\"Input dimension automatically detected from data: {in_dim_data}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Could not determine in_dim from data: {e}. Using default {in_dim_data}.\")\n",
    "else:\n",
    "    print(f\"train_loader or train_ds not available or empty. Using default in_dim: {in_dim_data}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    model = EEGGNN(\n",
    "        gnn_type=config[\"gnn_type\"],\n",
    "        in_dim=in_dim_data, # Use detected or default in_dim\n",
    "        hidden_dim=config[\"hidden_dim\"],\n",
    "        num_layers=config[\"num_layers\"],\n",
    "        dropout=config[\"dropout\"]\n",
    "        # num_classes is 1 by default for binary classification with BCEWithLogitsLoss\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=config[\"lr\"],\n",
    "        weight_decay=config[\"weight_decay\"]\n",
    "    )\n",
    "    print(\"Model and Optimizer instantiated.\")\n",
    "    print(model)\n",
    "except Exception as e:\n",
    "    print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    print(f\"ERROR instantiating model or optimizer: {e}\")\n",
    "    print(f\"This could be due to an issue with GNN type: '{config['gnn_type']}' or other parameters.\")\n",
    "    print(f\"!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!\")\n",
    "    model, optimizer = None, None # Ensure they are None if instantiation fails\n",
    "    # raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Focal_loss function defined.\n"
     ]
    }
   ],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# ║ 6a. Loss for imbalanced dataset                                      ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, alpha=0.25, gamma=2.0, reduction='mean', pos_weight=None):\n",
    "        \"\"\"\n",
    "        Focal Loss for binary classification.\n",
    "        Args:\n",
    "            alpha (float): Weighting factor for the rare class (e.g., if class 1 is rare, alpha for class 1).\n",
    "                           Can be thought of as similar to pos_weight's role but applied differently.\n",
    "                           Set to 0.25 for positive class if positive class is minority, \n",
    "                           or 1-alpha for negative class.\n",
    "                           If None, no alpha weighting is applied.\n",
    "            gamma (float): Focusing parameter. Higher values give more weight to hard, misclassified examples.\n",
    "            reduction (str): 'mean', 'sum', or 'none'.\n",
    "            pos_weight (torch.Tensor, optional): A weight of positive examples. If provided,\n",
    "                                                 this will be used by the underlying BCEWithLogitsLoss.\n",
    "                                                 If you use pos_weight here, you might not need alpha, or\n",
    "                                                 alpha can be used to further tune. For simplicity,\n",
    "                                                 you might start by using EITHER a good pos_weight OR alpha.\n",
    "                                                 If using alpha, pos_weight should ideally be None or 1.\n",
    "        \"\"\"\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.alpha = alpha\n",
    "        self.gamma = gamma\n",
    "        self.reduction = reduction\n",
    "        self.pos_weight = pos_weight\n",
    "\n",
    "    def forward(self, logits, targets):\n",
    "        # logits: model outputs BEFORE sigmoid (N,)\n",
    "        # targets: binary ground truth labels (N,)\n",
    "\n",
    "        # BCEWithLogitsLoss combines sigmoid and BCE loss for numerical stability\n",
    "        # If pos_weight is provided, it handles the weighting for class 1.\n",
    "        BCE_loss = F.binary_cross_entropy_with_logits(logits, targets, reduction='none', pos_weight=self.pos_weight)\n",
    "        \n",
    "        # Calculate pt (probability of the true class)\n",
    "        # targets * logits for class 1, (1-targets) * (-logits) for class 0\n",
    "        # p_t = exp(-BCE_loss) would be one way if BCE_loss was -log(pt)\n",
    "        # A more direct way with logits:\n",
    "        p = torch.sigmoid(logits)\n",
    "        # For true positives (targets=1), pt = p. For true negatives (targets=0), pt = 1-p\n",
    "        pt = p * targets + (1 - p) * (1 - targets)\n",
    "        \n",
    "        # Calculate Focal Loss\n",
    "        focal_term = (1.0 - pt).pow(self.gamma)\n",
    "        loss = focal_term * BCE_loss\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            # If targets=1, alpha_t = alpha. If targets=0, alpha_t = 1-alpha\n",
    "            # This assumes alpha is the weight for the positive class (targets=1)\n",
    "            alpha_t = self.alpha * targets + (1 - self.alpha) * (1 - targets)\n",
    "            loss = alpha_t * loss\n",
    "\n",
    "        if self.reduction == 'mean':\n",
    "            return loss.mean()\n",
    "        elif self.reduction == 'sum':\n",
    "            return loss.sum()\n",
    "        else:\n",
    "            return loss\n",
    "\n",
    "print(\"Focal_loss function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2NV8ZMm-8WQL",
    "outputId": "e185fcd5-b511-4824-babf-52604b8a9488"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_epoch function defined.\n"
     ]
    }
   ],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# ║ 6. OLD EPOCH FUNCTION (other loss)                                   ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "#def run_epoch(loader, train_mode=True):\n",
    "#    if model is None or optimizer is None:\n",
    "#        print(\"Model or optimizer not initialized. Skipping run_epoch.\")\n",
    "#        return {\"loss\": float('nan'), \"acc\": 0.0, \"f1\": 0.0, \"auroc\": 0.0}\n",
    "#\n",
    "#    model.train(train_mode) # Set model to train or eval mode\n",
    "#\n",
    "#    all_labels, all_predictions_prob, all_losses = [], [], []\n",
    "#\n",
    "#    for batch_data in loader:\n",
    "#        batch_data = batch_data.to(device)\n",
    "#\n",
    "#        # Forward pass\n",
    "#        logits = model(batch_data) # Shape: (batch_size,)\n",
    "#\n",
    "#        # Ensure labels are also (batch_size,) and float for BCEWithLogitsLoss\n",
    "#        pos_w = torch.tensor([4.16], device=device) # ~ num_class_0 / num_class_1\n",
    "#        labels = batch_data.y.squeeze().float()\n",
    "#        \n",
    "#        if logits.shape != labels.shape:\n",
    "#            # This can happen if batch size is 1 and squeeze leads to 0-dim tensor\n",
    "#            # Or if there's a mismatch in expected output.\n",
    "#            # For BCEWithLogits, both logits and labels should be [N] or [N, C]\n",
    "#            # Given model outputs (batch_size,), labels should be (batch_size,)\n",
    "#            print(f\"Warning: Shape mismatch. Logits: {logits.shape}, Labels: {labels.shape}. Attempting to reshape labels.\")\n",
    "#            if labels.ndim == 0 and logits.ndim == 1 and logits.shape[0] == 1: # labels is scalar, logits is [1]\n",
    "#                 labels = labels.unsqueeze(0)\n",
    "#            elif labels.ndim == 2 and labels.shape[1] == 1 and logits.ndim == 1 : # labels is [N,1]\n",
    "#                 labels = labels.squeeze(1)\n",
    "#            # Add more sophisticated checks if needed\n",
    "#            if logits.shape != labels.shape:\n",
    "#                  raise ValueError(f\"Corrected label shape {labels.shape} still does not match logits shape {logits.shape}\")\n",
    "#\n",
    "#\n",
    "#        loss = F.binary_cross_entropy_with_logits(logits, labels, pos_weight=pos_w)\n",
    "#\n",
    "#        if train_mode:\n",
    "#            optimizer.zero_grad()\n",
    "#            loss.backward()\n",
    "#            optimizer.step()\n",
    "#\n",
    "#        all_labels.append(labels.cpu().detach())\n",
    "#        all_predictions_prob.append(torch.sigmoid(logits).cpu().detach()) # Probabilities for AUROC\n",
    "#        all_losses.append(loss.item())\n",
    "#\n",
    "#    if not all_labels or not all_predictions_prob: # If loader was empty\n",
    "#        print(\"Warning: No data processed in run_epoch. Returning NaN/0 metrics.\")\n",
    "#        return {\"loss\": float('nan'), \"acc\": 0.0, \"f1\": 0.0, \"auroc\": 0.0}\n",
    "#\n",
    "#    # Concatenate all batch results\n",
    "#    final_labels = torch.cat(all_labels).numpy()\n",
    "#    final_predictions_prob = torch.cat(all_predictions_prob).numpy()\n",
    "#\n",
    "#    # Convert probabilities to binary predictions (0 or 1)\n",
    "#    final_predictions_binary = (final_predictions_prob > 0.5).astype(int)\n",
    "#\n",
    "#    # Inside run_epoch function in Cell 10,\n",
    "#    # right before: metrics = dict(...) or f1_score(...)\n",
    "#    \n",
    "#    print(f\"Unique values in final_labels: {np.unique(final_labels)}\")\n",
    "#    print(f\"dtype of final_labels: {final_labels.dtype}\")\n",
    "#    print(f\"Shape of final_labels: {final_labels.shape}\")\n",
    "#    print(f\"Unique values in final_predictions_binary: {np.unique(final_predictions_binary)}\")\n",
    "#    print(f\"dtype of final_predictions_binary: {final_predictions_binary.dtype}\")\n",
    "#    \n",
    "#    # Then your existing f1_score calculation:\n",
    "#    # f1_val = f1_score(final_labels, final_predictions_binary, zero_division=0) # Renamed to f1_val to avoid conflict if 'f1' is a key\n",
    "#    # metrics[\"f1\"] = f1_val\n",
    "#\n",
    "#    metrics = dict(\n",
    "#        loss = np.mean(all_losses),\n",
    "#        acc  = accuracy_score(final_labels, final_predictions_binary),\n",
    "#        f1   = f1_score(final_labels, final_predictions_binary, zero_division=0),\n",
    "#    )\n",
    "#    try:\n",
    "#        # AUROC requires at least one sample from each class in y_true for non-trivial calculation\n",
    "#        if len(np.unique(final_labels)) > 1:\n",
    "#            metrics[\"auroc\"] = roc_auc_score(final_labels, final_predictions_prob)\n",
    "#        else:\n",
    "#            metrics[\"auroc\"] = 0.0 # Or nan, or skip logging if only one class present\n",
    "#            # print(f\"Warning: Only one class present in labels for {'train' if train_mode else 'validation/test'} set. AUROC set to 0.0.\")\n",
    "#    except ValueError as e_auroc:\n",
    "#        # print(f\"Could not calculate AUROC: {e_auroc}. Setting to 0.0.\")\n",
    "#        metrics[\"auroc\"] = 0.0 # Or handle as NaN\n",
    "#\n",
    "#    return metrics\n",
    "#\n",
    "#print(\"run_epoch function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run_epoch_final with focal_loss function defined.\n"
     ]
    }
   ],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# ║ 6b. TRAIN / VALIDATE EPOCH FUNCTION                                  ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "import pandas as pd # Ensure pandas is imported if you use it for submission later\n",
    "\n",
    "\n",
    "def run_epoch(loader, model, train_mode=True, is_predict_mode=False, pos_weight_tensor=None): # Added is_predict_mode and pos_weight_tensor\n",
    "    if model is None: # Check if model is defined (should be)\n",
    "        print(\"Model not initialized. Skipping run_epoch.\")\n",
    "        if is_predict_mode:\n",
    "            return {\"predictions_prob\": np.array([]), \"predictions_binary\": np.array([]), \"signal_ids\": []}\n",
    "        return {\"loss\": float('nan'), \"acc\": 0.0, \"f1\": 0.0, \"auroc\": 0.0}\n",
    "\n",
    "    model.train(train_mode if not is_predict_mode else False) # Eval mode for prediction\n",
    "\n",
    "    all_predictions_prob_list = [] # Use lists to append tensors of varying batch sizes\n",
    "    all_signal_ids_list = []\n",
    "    \n",
    "    # Variables for metrics if not in predict mode\n",
    "    all_labels_list = []\n",
    "    all_losses_list = []\n",
    "\n",
    "    for batch_data in loader:\n",
    "        batch_data = batch_data.to(device)\n",
    "        \n",
    "        with torch.no_grad() if not train_mode or is_predict_mode else torch.enable_grad(): # No gradients needed for val or predict\n",
    "            logits = model(batch_data) # Shape: (batch_size,)\n",
    "\n",
    "        # Collect predictions (probabilities)\n",
    "        current_probs = torch.sigmoid(logits).cpu().detach()\n",
    "        all_predictions_prob_list.append(current_probs)\n",
    "\n",
    "        if hasattr(batch_data, 'signal_id'):\n",
    "            # Assuming signal_id is a list of strings already (from your GraphEEGDataset)\n",
    "            all_signal_ids_list.extend(batch_data.signal_id)\n",
    "\n",
    "        if not is_predict_mode:\n",
    "            labels = batch_data.y.squeeze().float()\n",
    "            if labels.ndim == 0: labels = labels.unsqueeze(0) # Ensure labels are at least 1D\n",
    "\n",
    "            # Handle potential -1 labels if they accidentally slip into validation\n",
    "            # This is a safeguard; ideally, val labels are 0 or 1.\n",
    "            valid_label_mask = (labels == 0) | (labels == 1)\n",
    "            if not torch.all(valid_label_mask):\n",
    "                print(f\"Warning: Invalid labels found in {'train' if train_mode else 'validation'} batch. Only using 0/1 for loss/metrics.\")\n",
    "            \n",
    "            valid_logits = logits[valid_label_mask]\n",
    "            valid_labels = labels[valid_label_mask]\n",
    "\n",
    "            if valid_logits.numel() > 0: # Ensure there are valid samples\n",
    "                current_pos_weight = None\n",
    "                if train_mode and pos_weight_tensor is not None:\n",
    "                     current_pos_weight = pos_weight_tensor\n",
    "                \n",
    "                focal_loss_fn = FocalLoss(alpha=None, gamma=2.0, reduction='mean', pos_weight=pos_weight_tensor_for_train)\n",
    "                #loss = F.binary_cross_entropy_with_logits(valid_logits, valid_labels, pos_weight=current_pos_weight)\n",
    "                loss = focal_loss_fn(valid_logits, valid_labels)\n",
    "                \n",
    "                all_losses_list.append(loss.item())\n",
    "                all_labels_list.append(valid_labels.cpu()) # Store only valid labels for metrics\n",
    "\n",
    "                if train_mode:\n",
    "                    optimizer.zero_grad()\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "            elif train_mode: # If no valid labels in a training batch, still need to handle optimizer\n",
    "                optimizer.zero_grad() # Clear gradients even if no backward pass occurred on this batch\n",
    "\n",
    "\n",
    "    if is_predict_mode:\n",
    "        if not all_predictions_prob_list: # Handle empty loader case\n",
    "            return {\"predictions_prob\": np.array([]), \"predictions_binary\": np.array([]), \"signal_ids\": []}\n",
    "            \n",
    "        final_predictions_prob = torch.cat(all_predictions_prob_list).numpy()\n",
    "        final_predictions_binary = (final_predictions_prob > 0.5).astype(int)\n",
    "        return {\n",
    "            \"predictions_prob\": final_predictions_prob,\n",
    "            \"predictions_binary\": final_predictions_binary,\n",
    "            \"signal_ids\": all_signal_ids_list # This is already a flat list of strings\n",
    "        }\n",
    "\n",
    "    # --- Metric Calculation for Training/Validation ---\n",
    "    if not all_labels_list: # If loader was empty or no valid labels processed\n",
    "        print(\"Warning: No data with valid labels (0 or 1) processed in run_epoch. Returning NaN/0 metrics.\")\n",
    "        return {\"loss\": float('nan'), \"acc\": 0.0, \"f1\": 0.0, \"auroc\": 0.0}\n",
    "\n",
    "    final_labels = torch.cat(all_labels_list).numpy()\n",
    "    # For metrics, use predictions corresponding to valid labels\n",
    "    # This requires aligning predictions with valid_labels if filtering occurred batch-wise.\n",
    "    # For simplicity here, assuming all_predictions_prob was for all samples,\n",
    "    # and we're evaluating on the subset that had valid labels.\n",
    "    # A more robust way would be to filter predictions alongside labels within the loop.\n",
    "    # However, since we expect val labels to be 0/1, this might be okay.\n",
    "    # Let's re-evaluate: we need predictions that correspond to final_labels\n",
    "    \n",
    "    # Re-think: if labels were filtered, predictions need to be filtered too.\n",
    "    # For now, let's assume validation set has proper 0/1 labels and this filtering is minor.\n",
    "    # The f1_score will be based on the collected valid labels.\n",
    "    # We need the predictions that corresponded to these valid labels.\n",
    "    # This part is tricky if `valid_label_mask` was used extensively.\n",
    "    # Assuming for validation, all labels are 0 or 1.\n",
    "    \n",
    "    final_predictions_prob_for_metrics = torch.cat(all_predictions_prob_list).numpy() # These are all predictions\n",
    "    if len(final_predictions_prob_for_metrics) != len(final_labels):\n",
    "        # This would happen if some batches had no valid labels but still contributed predictions.\n",
    "        # This indicates a need to more carefully collect predictions corresponding to valid labels.\n",
    "        # For now, this will likely cause an error in metric calculation if lengths differ.\n",
    "        # The robust way is to append to a list of predictions_for_metrics only when valid_labels exist.\n",
    "        # Let's assume for val, all labels are valid 0/1.\n",
    "        print(f\"Warning: Length mismatch between predictions ({len(final_predictions_prob_for_metrics)}) and labels ({len(final_labels)}). Metrics might be incorrect.\")\n",
    "        # Fallback or error needed here. For simplicity, let's proceed but acknowledge this.\n",
    "\n",
    "\n",
    "    final_predictions_binary_for_metrics = (final_predictions_prob_for_metrics > 0.5).astype(int)\n",
    "    \n",
    "    # Ensure final_labels only contains 0 and 1 for sklearn metrics if this check is still needed\n",
    "    if not np.all(np.isin(final_labels, [0, 1])):\n",
    "        print(\"Error: final_labels for metric calculation still contain non-binary values.\")\n",
    "        return {\"loss\": np.mean(all_losses_list) if all_losses_list else float('nan'), \"acc\": 0.0, \"f1\": 0.0, \"auroc\": 0.0}\n",
    "\n",
    "    metrics = dict(\n",
    "        loss = np.mean(all_losses_list) if all_losses_list else float('nan'),\n",
    "        acc  = accuracy_score(final_labels, final_predictions_binary_for_metrics),\n",
    "        f1   = f1_score(final_labels, final_predictions_binary_for_metrics, zero_division=0), # Default average='binary' if labels are [0,1]\n",
    "    )\n",
    "    try:\n",
    "        if len(np.unique(final_labels)) > 1: # AUROC requires at least one sample from each class\n",
    "            metrics[\"auroc\"] = roc_auc_score(final_labels, final_predictions_prob_for_metrics)\n",
    "        else:\n",
    "            metrics[\"auroc\"] = 0.0 \n",
    "    except ValueError as e_auroc:\n",
    "        metrics[\"auroc\"] = 0.0\n",
    "    return metrics\n",
    "\n",
    "print(\"run_epoch_final with focal_loss function defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QZAqMwpW8X1r",
    "outputId": "2d0966bc-76f2-4a8c-9454-a2689d4ad9e8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Training for 25 epochs ---\n"
     ]
    },
    {
     "ename": "Error",
     "evalue": "You must call wandb.init() before wandb.watch()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mError\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[177], line 9\u001b[0m\n\u001b[1;32m      6\u001b[0m best_model_state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m--- Starting Training for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m epochs ---\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 9\u001b[0m \u001b[43mwandb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwatch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mall\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlog_freq\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m# Log model gradients and parameters\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, config[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mepochs\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m     12\u001b[0m     epoch_start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/wandb/sdk/lib/preinit.py:36\u001b[0m, in \u001b[0;36mPreInitCallable.<locals>.preinit_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mpreinit_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs: Any, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs: Any) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m---> 36\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m wandb\u001b[38;5;241m.\u001b[39mError(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must call wandb.init() before \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mError\u001b[0m: You must call wandb.init() before wandb.watch()"
     ]
    }
   ],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# ║ 7. MAIN TRAINING LOOP (just one)                                     ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "if train_loader is not None and val_loader is not None and model is not None:\n",
    "    best_val_f1 = 0\n",
    "    best_model_state = None\n",
    "\n",
    "    print(f\"\\n--- Starting Training for {config['epochs']} epochs ---\")\n",
    "    wandb.watch(model, log=\"all\", log_freq=100) # Log model gradients and parameters\n",
    "\n",
    "    for epoch in range(1, config[\"epochs\"] + 1):\n",
    "        epoch_start_time = time.time()\n",
    "\n",
    "        train_metrics = run_epoch(train_loader, train_mode=True)\n",
    "        val_metrics   = run_epoch(val_loader,   train_mode=False)\n",
    "\n",
    "        epoch_duration = time.time() - epoch_start_time\n",
    "\n",
    "        # Log metrics to WandB\n",
    "        wandb_log_data = {}\n",
    "        for k, v in train_metrics.items(): wandb_log_data[f\"train_{k}\"] = v\n",
    "        for k, v in val_metrics.items(): wandb_log_data[f\"val_{k}\"] = v\n",
    "        wandb_log_data[\"epoch\"] = epoch\n",
    "        wandb_log_data[\"epoch_duration_sec\"] = epoch_duration\n",
    "        wandb.log(wandb_log_data)\n",
    "\n",
    "        print(f\"[Epoch {epoch:03d}/{config['epochs']}] \"\n",
    "              f\"Train Loss: {train_metrics['loss']:.4f}, Train F1: {train_metrics['f1']:.4f}, Train AUROC: {train_metrics['auroc']:.4f} | \"\n",
    "              f\"Val Loss: {val_metrics['loss']:.4f}, Val F1: {val_metrics['f1']:.4f}, Val AUROC: {val_metrics['auroc']:.4f} | \"\n",
    "              f\"Time: {epoch_duration:.2f}s\")\n",
    "\n",
    "        if val_metrics[\"f1\"] > best_val_f1:\n",
    "            best_val_f1 = val_metrics[\"f1\"]\n",
    "            best_model_state = model.state_dict().copy() # Get a copy of the state dict\n",
    "            print(f\"  New best validation F1: {best_val_f1:.4f}. Saving model state.\")\n",
    "\n",
    "            # Save the best model checkpoint to Google Drive\n",
    "            best_model_filename = f\"best_model_{config['gnn_type']}_{config['edge_variant']}_epoch{epoch}.pt\"\n",
    "            best_model_save_path = os.path.join(config[\"base_dir\"], best_model_filename)\n",
    "            try:\n",
    "                torch.save(best_model_state, best_model_save_path)\n",
    "                print(f\"  Best model saved to: {best_model_save_path}\")\n",
    "                # Optionally, tell WandB about the saved model artifact\n",
    "                # best_model_artifact = wandb.Artifact(f\"{config['gnn_type']}-{config['edge_variant']}-best-model\", type=\"model\")\n",
    "                # best_model_artifact.add_file(best_model_save_path)\n",
    "                # wandb.log_artifact(best_model_artifact)\n",
    "\n",
    "            except Exception as e_save:\n",
    "                print(f\"  Error saving model: {e_save}\")\n",
    "\n",
    "    print(f\"\\n--- Training Finished ---\")\n",
    "    print(f\"Best Validation F1 achieved: {best_val_f1:.4f}\")\n",
    "\n",
    "    # Save the final best model state explicitly if not done above or if you want a generic name\n",
    "    if best_model_state is not None:\n",
    "        final_best_model_filename = f\"best_overall_model_{config['gnn_type']}_{config['edge_variant']}.pt\"\n",
    "        final_best_model_save_path = os.path.join(config[\"base_dir\"], final_best_model_filename)\n",
    "        try:\n",
    "            torch.save(best_model_state, final_best_model_save_path)\n",
    "            print(f\"Final best model state saved to: {final_best_model_save_path}\")\n",
    "        except Exception as e_save_final:\n",
    "            print(f\"  Error saving final best model: {e_save_final}\")\n",
    "    else:\n",
    "        print(\"No best model state was saved during training (e.g., validation F1 never improved or training was skipped).\")\n",
    "\n",
    "else:\n",
    "    print(\"Skipping training loop due to earlier errors (loaders or model not available).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 265,
     "referenced_widgets": [
      "dfc1932369d84d048d3ceec60063a5b8",
      "d36aee82f30f48929b662bac0b11e099",
      "a4626d75a1dc4c56b583423f4218dead",
      "836064a4fbf44e08880d77ac0ea0093e",
      "3ab7bfaa6b1f4e19a0b11d5ba8062ede",
      "e30a199b9fe7407695adb21a7c533cf3",
      "e4a9eeefebc64c67b4ba57ccec7a14d9",
      "d608471227b24cf2ad55f8a1687556f2",
      "51cf56ca4d5f489ca8bead2d4c33ad3e",
      "56ff9c963f9945269d806965b7df1548",
      "2072151e224746a792d7ad2960d0ac3d"
     ]
    },
    "id": "gzfCnJzM8aBL",
    "outputId": "95d7cbed-36ad-47ad-ff3e-d3b03ad94ce1"
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for EEGGNN:\n\tMissing key(s) in state_dict: \"convs.0.lin_l.weight\", \"convs.0.lin_l.bias\", \"convs.0.lin_r.weight\", \"convs.1.lin_l.weight\", \"convs.1.lin_l.bias\", \"convs.1.lin_r.weight\", \"convs.2.lin_l.weight\", \"convs.2.lin_l.bias\", \"convs.2.lin_r.weight\", \"convs.3.lin_l.weight\", \"convs.3.lin_l.bias\", \"convs.3.lin_r.weight\", \"convs.4.lin_l.weight\", \"convs.4.lin_l.bias\", \"convs.4.lin_r.weight\", \"convs.5.lin_l.weight\", \"convs.5.lin_l.bias\", \"convs.5.lin_r.weight\". \n\tUnexpected key(s) in state_dict: \"convs.0.bias\", \"convs.0.lin.weight\", \"convs.1.bias\", \"convs.1.lin.weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[156], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m model_load_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbest_overall_model_gcn_grid.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# ... (load your best model, ensure it's on the correct device and in eval mode)\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_state_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_load_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmap_location\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m# ...\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# --- Load test data (it will have -1 labels from EEG_nml.py) ---\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# SCRIPT_PATH and config[\"base_dir\"] should be defined\u001b[39;00m\n",
      "File \u001b[0;32m/opt/jlab-env/lib/python3.12/site-packages/torch/nn/modules/module.py:2584\u001b[0m, in \u001b[0;36mModule.load_state_dict\u001b[0;34m(self, state_dict, strict, assign)\u001b[0m\n\u001b[1;32m   2576\u001b[0m         error_msgs\u001b[38;5;241m.\u001b[39minsert(\n\u001b[1;32m   2577\u001b[0m             \u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m   2578\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMissing key(s) in state_dict: \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m. \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2579\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mk\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m missing_keys)\n\u001b[1;32m   2580\u001b[0m             ),\n\u001b[1;32m   2581\u001b[0m         )\n\u001b[1;32m   2583\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(error_msgs) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m-> 2584\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[1;32m   2585\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError(s) in loading state_dict for \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[1;32m   2586\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\t\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(error_msgs)\n\u001b[1;32m   2587\u001b[0m         )\n\u001b[1;32m   2588\u001b[0m     )\n\u001b[1;32m   2589\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _IncompatibleKeys(missing_keys, unexpected_keys)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for EEGGNN:\n\tMissing key(s) in state_dict: \"convs.0.lin_l.weight\", \"convs.0.lin_l.bias\", \"convs.0.lin_r.weight\", \"convs.1.lin_l.weight\", \"convs.1.lin_l.bias\", \"convs.1.lin_r.weight\", \"convs.2.lin_l.weight\", \"convs.2.lin_l.bias\", \"convs.2.lin_r.weight\", \"convs.3.lin_l.weight\", \"convs.3.lin_l.bias\", \"convs.3.lin_r.weight\", \"convs.4.lin_l.weight\", \"convs.4.lin_l.bias\", \"convs.4.lin_r.weight\", \"convs.5.lin_l.weight\", \"convs.5.lin_l.bias\", \"convs.5.lin_r.weight\". \n\tUnexpected key(s) in state_dict: \"convs.0.bias\", \"convs.0.lin.weight\", \"convs.1.bias\", \"convs.1.lin.weight\". "
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "id": "2HduN-mb8bXF",
    "outputId": "5cef745f-b34b-4a47-9c22-8ceeb65a89ba"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Bu8SZ-He8efa"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "===================================\n",
      "🚀 STARTING EXPERIMENT: gat_grid_layers4_lr1e-05\n",
      "===================================\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">still-dream-40</strong> at: <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/x04vg56z' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/x04vg56z</a><br> View project at: <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250528_120258-x04vg56z/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/my_notebooks/EEG_nml/wandb/run-20250528_120330-zbo9os6b</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/zbo9os6b' target=\"_blank\">gat_grid_layers4_lr1e-05</a></strong> to <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/zbo9os6b' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/zbo9os6b</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB initialized for: gat_grid_layers4_lr1e-05\n",
      "Loading edge index: ../EEG_nml/edge_index/edge_index_grid.pt\n",
      "Edge index 'grid' loaded. Shape: torch.Size([2, 48])\n",
      "DataLoaders created. Train: 10394, Val: 2599\n",
      "Model (gat) & Optimizer instantiated.\n",
      "--- 🏋️ Starting Training for: gat_grid_layers4_lr1e-05 ---\n",
      "[Epoch 001/15] Tr Loss: 0.2768, Tr F1: 0.3567, Tr AUROC: 0.6348 | Val Loss: 0.2501, Val F1: 0.4191, Val AUROC: 0.7466 | Time: 8.10s\n",
      "  ⭐ New best validation F1: 0.4191 for gat_grid_layers4_lr1e-05.\n",
      "[Epoch 002/15] Tr Loss: 0.2547, Tr F1: 0.4147, Tr AUROC: 0.7143 | Val Loss: 0.2411, Val F1: 0.4372, Val AUROC: 0.7616 | Time: 8.42s\n",
      "  ⭐ New best validation F1: 0.4372 for gat_grid_layers4_lr1e-05.\n",
      "[Epoch 003/15] Tr Loss: 0.2481, Tr F1: 0.4274, Tr AUROC: 0.7323 | Val Loss: 0.2374, Val F1: 0.4564, Val AUROC: 0.7686 | Time: 8.11s\n",
      "  ⭐ New best validation F1: 0.4564 for gat_grid_layers4_lr1e-05.\n",
      "[Epoch 004/15] Tr Loss: 0.2473, Tr F1: 0.4281, Tr AUROC: 0.7292 | Val Loss: 0.2391, Val F1: 0.4446, Val AUROC: 0.7550 | Time: 8.10s\n",
      "[Epoch 005/15] Tr Loss: 0.2445, Tr F1: 0.4381, Tr AUROC: 0.7380 | Val Loss: 0.2371, Val F1: 0.4377, Val AUROC: 0.7552 | Time: 8.03s\n",
      "[Epoch 006/15] Tr Loss: 0.2438, Tr F1: 0.4439, Tr AUROC: 0.7395 | Val Loss: 0.2349, Val F1: 0.4697, Val AUROC: 0.7694 | Time: 8.05s\n",
      "  ⭐ New best validation F1: 0.4697 for gat_grid_layers4_lr1e-05.\n",
      "[Epoch 007/15] Tr Loss: 0.2453, Tr F1: 0.4286, Tr AUROC: 0.7345 | Val Loss: 0.2385, Val F1: 0.4587, Val AUROC: 0.7532 | Time: 8.00s\n",
      "[Epoch 008/15] Tr Loss: 0.2439, Tr F1: 0.4367, Tr AUROC: 0.7408 | Val Loss: 0.2357, Val F1: 0.4474, Val AUROC: 0.7623 | Time: 7.95s\n",
      "[Epoch 009/15] Tr Loss: 0.2438, Tr F1: 0.4441, Tr AUROC: 0.7412 | Val Loss: 0.2357, Val F1: 0.4233, Val AUROC: 0.7570 | Time: 7.89s\n",
      "[Epoch 010/15] Tr Loss: 0.2429, Tr F1: 0.4321, Tr AUROC: 0.7420 | Val Loss: 0.2351, Val F1: 0.4554, Val AUROC: 0.7617 | Time: 8.00s\n",
      "[Epoch 011/15] Tr Loss: 0.2408, Tr F1: 0.4500, Tr AUROC: 0.7502 | Val Loss: 0.2326, Val F1: 0.4613, Val AUROC: 0.7726 | Time: 7.91s\n",
      "[Epoch 012/15] Tr Loss: 0.2414, Tr F1: 0.4424, Tr AUROC: 0.7488 | Val Loss: 0.2355, Val F1: 0.4505, Val AUROC: 0.7560 | Time: 7.98s\n",
      "[Epoch 013/15] Tr Loss: 0.2398, Tr F1: 0.4507, Tr AUROC: 0.7519 | Val Loss: 0.2352, Val F1: 0.4257, Val AUROC: 0.7523 | Time: 8.02s\n",
      "[Epoch 014/15] Tr Loss: 0.2390, Tr F1: 0.4456, Tr AUROC: 0.7541 | Val Loss: 0.2330, Val F1: 0.4696, Val AUROC: 0.7666 | Time: 7.93s\n",
      "[Epoch 015/15] Tr Loss: 0.2412, Tr F1: 0.4347, Tr AUROC: 0.7469 | Val Loss: 0.2339, Val F1: 0.4642, Val AUROC: 0.7613 | Time: 8.02s\n",
      "--- ✅ Training Finished for gat_grid_layers4_lr1e-05. Best Val F1: 0.4697 ---\n",
      "Best model for gat_grid_layers4_lr1e-05 saved to: ../EEG_nml/best_model_gat_grid_layers4_lr1e-05.pt\n",
      "\n",
      "--- 🧪 Generating Predictions for gat_grid_layers4_lr1e-05 ---\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='0', CREATED composite_id='pqejgcvm_s001_t000_0'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='1', CREATED composite_id='pqejgcvm_s001_t000_1'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='2', CREATED composite_id='pqejgcvm_s001_t000_2'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='3', CREATED composite_id='pqejgcvm_s001_t000_3'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='4', CREATED composite_id='pqejgcvm_s001_t000_4'\n",
      "Submission CSV for gat_grid_layers4_lr1e-05 saved to: ../EEG_nml/submission_gat_grid_layers4_lr1e-05.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇█▇██▇█████</td></tr><tr><td>train_auroc</td><td>▁▆▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_f1</td><td>▁▅▆▆▇▇▆▇█▇█▇██▇</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▅▆▅▇▇▅▆▆▇█▇▇█</td></tr><tr><td>val_auroc</td><td>▁▅▇▃▃▇▃▅▄▅█▄▃▆▅</td></tr><tr><td>val_f1</td><td>▁▄▆▅▄█▆▅▂▆▇▅▂█▇</td></tr><tr><td>val_loss</td><td>█▄▃▄▃▂▃▂▂▂▁▂▂▁▂</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>0.65461</td></tr><tr><td>train_auroc</td><td>0.74691</td></tr><tr><td>train_f1</td><td>0.43473</td></tr><tr><td>train_loss</td><td>0.24121</td></tr><tr><td>val_acc</td><td>0.72913</td></tr><tr><td>val_auroc</td><td>0.76126</td></tr><tr><td>val_f1</td><td>0.46423</td></tr><tr><td>val_loss</td><td>0.23394</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gat_grid_layers4_lr1e-05</strong> at: <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/zbo9os6b' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/zbo9os6b</a><br> View project at: <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project</a><br>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250528_120330-zbo9os6b/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "🏁 COMPLETED EXPERIMENT: gat_grid_layers4_lr1e-05\n",
      "===================================\n",
      "\n",
      "\n",
      "===================================\n",
      "🚀 STARTING EXPERIMENT: gat_knn_layers4_lr1e-05\n",
      "===================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/my_notebooks/EEG_nml/wandb/run-20250528_120537-8qncm3cq</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/8qncm3cq' target=\"_blank\">gat_knn_layers4_lr1e-05</a></strong> to <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/8qncm3cq' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/8qncm3cq</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB initialized for: gat_knn_layers4_lr1e-05\n",
      "Loading edge index: ../EEG_nml/edge_index/edge_index_knn.pt\n",
      "Edge index 'knn' loaded. Shape: torch.Size([2, 152])\n",
      "DataLoaders created. Train: 10394, Val: 2599\n",
      "Model (gat) & Optimizer instantiated.\n",
      "--- 🏋️ Starting Training for: gat_knn_layers4_lr1e-05 ---\n",
      "[Epoch 001/15] Tr Loss: 0.2767, Tr F1: 0.3559, Tr AUROC: 0.6369 | Val Loss: 0.2494, Val F1: 0.4126, Val AUROC: 0.7512 | Time: 8.06s\n",
      "  ⭐ New best validation F1: 0.4126 for gat_knn_layers4_lr1e-05.\n",
      "[Epoch 002/15] Tr Loss: 0.2538, Tr F1: 0.4138, Tr AUROC: 0.7154 | Val Loss: 0.2405, Val F1: 0.4422, Val AUROC: 0.7647 | Time: 8.09s\n",
      "  ⭐ New best validation F1: 0.4422 for gat_knn_layers4_lr1e-05.\n",
      "[Epoch 003/15] Tr Loss: 0.2475, Tr F1: 0.4287, Tr AUROC: 0.7330 | Val Loss: 0.2372, Val F1: 0.4514, Val AUROC: 0.7663 | Time: 7.91s\n",
      "  ⭐ New best validation F1: 0.4514 for gat_knn_layers4_lr1e-05.\n",
      "[Epoch 004/15] Tr Loss: 0.2461, Tr F1: 0.4340, Tr AUROC: 0.7322 | Val Loss: 0.2383, Val F1: 0.4481, Val AUROC: 0.7590 | Time: 8.30s\n",
      "[Epoch 005/15] Tr Loss: 0.2439, Tr F1: 0.4399, Tr AUROC: 0.7397 | Val Loss: 0.2362, Val F1: 0.4659, Val AUROC: 0.7627 | Time: 8.00s\n",
      "  ⭐ New best validation F1: 0.4659 for gat_knn_layers4_lr1e-05.\n",
      "[Epoch 006/15] Tr Loss: 0.2433, Tr F1: 0.4411, Tr AUROC: 0.7397 | Val Loss: 0.2347, Val F1: 0.4748, Val AUROC: 0.7697 | Time: 8.01s\n",
      "  ⭐ New best validation F1: 0.4748 for gat_knn_layers4_lr1e-05.\n",
      "[Epoch 007/15] Tr Loss: 0.2447, Tr F1: 0.4293, Tr AUROC: 0.7354 | Val Loss: 0.2380, Val F1: 0.4619, Val AUROC: 0.7562 | Time: 8.00s\n",
      "[Epoch 008/15] Tr Loss: 0.2432, Tr F1: 0.4401, Tr AUROC: 0.7420 | Val Loss: 0.2350, Val F1: 0.4518, Val AUROC: 0.7645 | Time: 7.95s\n",
      "[Epoch 009/15] Tr Loss: 0.2427, Tr F1: 0.4456, Tr AUROC: 0.7442 | Val Loss: 0.2348, Val F1: 0.4449, Val AUROC: 0.7619 | Time: 7.89s\n",
      "[Epoch 010/15] Tr Loss: 0.2422, Tr F1: 0.4340, Tr AUROC: 0.7433 | Val Loss: 0.2345, Val F1: 0.4772, Val AUROC: 0.7634 | Time: 7.97s\n",
      "  ⭐ New best validation F1: 0.4772 for gat_knn_layers4_lr1e-05.\n",
      "[Epoch 011/15] Tr Loss: 0.2400, Tr F1: 0.4477, Tr AUROC: 0.7517 | Val Loss: 0.2324, Val F1: 0.4804, Val AUROC: 0.7739 | Time: 7.96s\n",
      "  ⭐ New best validation F1: 0.4804 for gat_knn_layers4_lr1e-05.\n",
      "[Epoch 012/15] Tr Loss: 0.2403, Tr F1: 0.4450, Tr AUROC: 0.7504 | Val Loss: 0.2337, Val F1: 0.4354, Val AUROC: 0.7618 | Time: 7.95s\n",
      "[Epoch 013/15] Tr Loss: 0.2388, Tr F1: 0.4540, Tr AUROC: 0.7543 | Val Loss: 0.2337, Val F1: 0.4520, Val AUROC: 0.7601 | Time: 8.00s\n",
      "[Epoch 014/15] Tr Loss: 0.2375, Tr F1: 0.4510, Tr AUROC: 0.7570 | Val Loss: 0.2322, Val F1: 0.4572, Val AUROC: 0.7667 | Time: 8.03s\n",
      "[Epoch 015/15] Tr Loss: 0.2398, Tr F1: 0.4381, Tr AUROC: 0.7503 | Val Loss: 0.2328, Val F1: 0.4903, Val AUROC: 0.7652 | Time: 7.98s\n",
      "  ⭐ New best validation F1: 0.4903 for gat_knn_layers4_lr1e-05.\n",
      "--- ✅ Training Finished for gat_knn_layers4_lr1e-05. Best Val F1: 0.4903 ---\n",
      "Best model for gat_knn_layers4_lr1e-05 saved to: ../EEG_nml/best_model_gat_knn_layers4_lr1e-05.pt\n",
      "\n",
      "--- 🧪 Generating Predictions for gat_knn_layers4_lr1e-05 ---\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='0', CREATED composite_id='pqejgcvm_s001_t000_0'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='1', CREATED composite_id='pqejgcvm_s001_t000_1'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='2', CREATED composite_id='pqejgcvm_s001_t000_2'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='3', CREATED composite_id='pqejgcvm_s001_t000_3'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='4', CREATED composite_id='pqejgcvm_s001_t000_4'\n",
      "Submission CSV for gat_knn_layers4_lr1e-05 saved to: ../EEG_nml/submission_gat_knn_layers4_lr1e-05.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇▇▇██▇█████</td></tr><tr><td>train_auroc</td><td>▁▆▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train_f1</td><td>▁▅▆▇▇▇▆▇▇▇█▇██▇</td></tr><tr><td>train_loss</td><td>█▄▃▃▂▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▅▅▆▇▇▅▇▇█▇▆▇█</td></tr><tr><td>val_auroc</td><td>▁▅▆▃▅▇▃▅▄▅█▄▄▆▅</td></tr><tr><td>val_f1</td><td>▁▄▅▄▆▇▅▅▄▇▇▃▅▅█</td></tr><tr><td>val_loss</td><td>█▄▃▃▃▂▃▂▂▂▁▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>0.66107</td></tr><tr><td>train_auroc</td><td>0.75027</td></tr><tr><td>train_f1</td><td>0.43812</td></tr><tr><td>train_loss</td><td>0.23977</td></tr><tr><td>val_acc</td><td>0.72797</td></tr><tr><td>val_auroc</td><td>0.76523</td></tr><tr><td>val_f1</td><td>0.49027</td></tr><tr><td>val_loss</td><td>0.23276</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gat_knn_layers4_lr1e-05</strong> at: <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/8qncm3cq' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/8qncm3cq</a><br> View project at: <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project</a><br>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250528_120537-8qncm3cq/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "🏁 COMPLETED EXPERIMENT: gat_knn_layers4_lr1e-05\n",
      "===================================\n",
      "\n",
      "\n",
      "===================================\n",
      "🚀 STARTING EXPERIMENT: sage_grid_layers4_lr1e-05\n",
      "===================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/my_notebooks/EEG_nml/wandb/run-20250528_120742-08fctp7q</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/08fctp7q' target=\"_blank\">sage_grid_layers4_lr1e-05</a></strong> to <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/08fctp7q' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/08fctp7q</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB initialized for: sage_grid_layers4_lr1e-05\n",
      "Loading edge index: ../EEG_nml/edge_index/edge_index_grid.pt\n",
      "Edge index 'grid' loaded. Shape: torch.Size([2, 48])\n",
      "DataLoaders created. Train: 10394, Val: 2599\n",
      "Model (sage) & Optimizer instantiated.\n",
      "--- 🏋️ Starting Training for: sage_grid_layers4_lr1e-05 ---\n",
      "[Epoch 001/15] Tr Loss: 0.5666, Tr F1: 0.2584, Tr AUROC: 0.4199 | Val Loss: 0.3724, Val F1: 0.2554, Val AUROC: 0.4125 | Time: 7.24s\n",
      "  ⭐ New best validation F1: 0.2554 for sage_grid_layers4_lr1e-05.\n",
      "[Epoch 002/15] Tr Loss: 0.3327, Tr F1: 0.3020, Tr AUROC: 0.4751 | Val Loss: 0.2966, Val F1: 0.3010, Val AUROC: 0.5445 | Time: 7.22s\n",
      "  ⭐ New best validation F1: 0.3010 for sage_grid_layers4_lr1e-05.\n",
      "[Epoch 003/15] Tr Loss: 0.2925, Tr F1: 0.3262, Tr AUROC: 0.5690 | Val Loss: 0.2777, Val F1: 0.3379, Val AUROC: 0.6215 | Time: 7.23s\n",
      "  ⭐ New best validation F1: 0.3379 for sage_grid_layers4_lr1e-05.\n",
      "[Epoch 004/15] Tr Loss: 0.2781, Tr F1: 0.3526, Tr AUROC: 0.6162 | Val Loss: 0.2685, Val F1: 0.3765, Val AUROC: 0.6514 | Time: 7.27s\n",
      "  ⭐ New best validation F1: 0.3765 for sage_grid_layers4_lr1e-05.\n",
      "[Epoch 005/15] Tr Loss: 0.2744, Tr F1: 0.3562, Tr AUROC: 0.6222 | Val Loss: 0.2647, Val F1: 0.3764, Val AUROC: 0.6640 | Time: 7.30s\n",
      "[Epoch 006/15] Tr Loss: 0.2705, Tr F1: 0.3736, Tr AUROC: 0.6439 | Val Loss: 0.2621, Val F1: 0.3978, Val AUROC: 0.6742 | Time: 7.28s\n",
      "  ⭐ New best validation F1: 0.3978 for sage_grid_layers4_lr1e-05.\n",
      "[Epoch 007/15] Tr Loss: 0.2673, Tr F1: 0.3738, Tr AUROC: 0.6496 | Val Loss: 0.2617, Val F1: 0.4036, Val AUROC: 0.6833 | Time: 7.59s\n",
      "  ⭐ New best validation F1: 0.4036 for sage_grid_layers4_lr1e-05.\n",
      "[Epoch 008/15] Tr Loss: 0.2653, Tr F1: 0.3793, Tr AUROC: 0.6616 | Val Loss: 0.2593, Val F1: 0.4012, Val AUROC: 0.6944 | Time: 7.30s\n",
      "[Epoch 009/15] Tr Loss: 0.2657, Tr F1: 0.3809, Tr AUROC: 0.6626 | Val Loss: 0.2575, Val F1: 0.4365, Val AUROC: 0.6981 | Time: 7.27s\n",
      "  ⭐ New best validation F1: 0.4365 for sage_grid_layers4_lr1e-05.\n",
      "[Epoch 010/15] Tr Loss: 0.2639, Tr F1: 0.3919, Tr AUROC: 0.6702 | Val Loss: 0.2571, Val F1: 0.4414, Val AUROC: 0.7042 | Time: 7.29s\n",
      "  ⭐ New best validation F1: 0.4414 for sage_grid_layers4_lr1e-05.\n",
      "[Epoch 011/15] Tr Loss: 0.2645, Tr F1: 0.3845, Tr AUROC: 0.6690 | Val Loss: 0.2564, Val F1: 0.4281, Val AUROC: 0.7033 | Time: 7.30s\n",
      "[Epoch 012/15] Tr Loss: 0.2643, Tr F1: 0.3885, Tr AUROC: 0.6697 | Val Loss: 0.2544, Val F1: 0.4295, Val AUROC: 0.7127 | Time: 7.31s\n",
      "[Epoch 013/15] Tr Loss: 0.2623, Tr F1: 0.3990, Tr AUROC: 0.6763 | Val Loss: 0.2537, Val F1: 0.4241, Val AUROC: 0.7129 | Time: 7.32s\n",
      "[Epoch 014/15] Tr Loss: 0.2617, Tr F1: 0.3994, Tr AUROC: 0.6784 | Val Loss: 0.2528, Val F1: 0.4334, Val AUROC: 0.7149 | Time: 7.29s\n",
      "[Epoch 015/15] Tr Loss: 0.2613, Tr F1: 0.3992, Tr AUROC: 0.6810 | Val Loss: 0.2525, Val F1: 0.4392, Val AUROC: 0.7153 | Time: 7.31s\n",
      "--- ✅ Training Finished for sage_grid_layers4_lr1e-05. Best Val F1: 0.4414 ---\n",
      "Best model for sage_grid_layers4_lr1e-05 saved to: ../EEG_nml/best_model_sage_grid_layers4_lr1e-05.pt\n",
      "\n",
      "--- 🧪 Generating Predictions for sage_grid_layers4_lr1e-05 ---\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='0', CREATED composite_id='pqejgcvm_s001_t000_0'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='1', CREATED composite_id='pqejgcvm_s001_t000_1'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='2', CREATED composite_id='pqejgcvm_s001_t000_2'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='3', CREATED composite_id='pqejgcvm_s001_t000_3'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='4', CREATED composite_id='pqejgcvm_s001_t000_4'\n",
      "Submission CSV for sage_grid_layers4_lr1e-05 saved to: ../EEG_nml/submission_sage_grid_layers4_lr1e-05.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▂▁▃▅▆▇▇████████</td></tr><tr><td>train_auroc</td><td>▁▂▅▆▆▇▇▇███████</td></tr><tr><td>train_f1</td><td>▁▃▄▆▆▇▇▇▇█▇▇███</td></tr><tr><td>train_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▂▁▄▆▆▇▇▇███▇▇█▇</td></tr><tr><td>val_auroc</td><td>▁▄▆▇▇▇▇████████</td></tr><tr><td>val_f1</td><td>▁▃▄▆▆▆▇▆██▇█▇██</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>0.61912</td></tr><tr><td>train_auroc</td><td>0.681</td></tr><tr><td>train_f1</td><td>0.39921</td></tr><tr><td>train_loss</td><td>0.26134</td></tr><tr><td>val_acc</td><td>0.62178</td></tr><tr><td>val_auroc</td><td>0.71529</td></tr><tr><td>val_f1</td><td>0.43925</td></tr><tr><td>val_loss</td><td>0.25252</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sage_grid_layers4_lr1e-05</strong> at: <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/08fctp7q' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/08fctp7q</a><br> View project at: <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project</a><br>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250528_120742-08fctp7q/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "🏁 COMPLETED EXPERIMENT: sage_grid_layers4_lr1e-05\n",
      "===================================\n",
      "\n",
      "\n",
      "===================================\n",
      "🚀 STARTING EXPERIMENT: sage_knn_layers4_lr1e-05\n",
      "===================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/my_notebooks/EEG_nml/wandb/run-20250528_120937-35jwz6ja</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/35jwz6ja' target=\"_blank\">sage_knn_layers4_lr1e-05</a></strong> to <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/35jwz6ja' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/35jwz6ja</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB initialized for: sage_knn_layers4_lr1e-05\n",
      "Loading edge index: ../EEG_nml/edge_index/edge_index_knn.pt\n",
      "Edge index 'knn' loaded. Shape: torch.Size([2, 152])\n",
      "DataLoaders created. Train: 10394, Val: 2599\n",
      "Model (sage) & Optimizer instantiated.\n",
      "--- 🏋️ Starting Training for: sage_knn_layers4_lr1e-05 ---\n",
      "[Epoch 001/15] Tr Loss: 0.5724, Tr F1: 0.2574, Tr AUROC: 0.4201 | Val Loss: 0.3725, Val F1: 0.2543, Val AUROC: 0.4146 | Time: 7.34s\n",
      "  ⭐ New best validation F1: 0.2543 for sage_knn_layers4_lr1e-05.\n",
      "[Epoch 002/15] Tr Loss: 0.3329, Tr F1: 0.3028, Tr AUROC: 0.4765 | Val Loss: 0.2969, Val F1: 0.3073, Val AUROC: 0.5502 | Time: 7.26s\n",
      "  ⭐ New best validation F1: 0.3073 for sage_knn_layers4_lr1e-05.\n",
      "[Epoch 003/15] Tr Loss: 0.2917, Tr F1: 0.3246, Tr AUROC: 0.5744 | Val Loss: 0.2772, Val F1: 0.3368, Val AUROC: 0.6210 | Time: 7.25s\n",
      "  ⭐ New best validation F1: 0.3368 for sage_knn_layers4_lr1e-05.\n",
      "[Epoch 004/15] Tr Loss: 0.2774, Tr F1: 0.3564, Tr AUROC: 0.6215 | Val Loss: 0.2679, Val F1: 0.3724, Val AUROC: 0.6561 | Time: 7.30s\n",
      "  ⭐ New best validation F1: 0.3724 for sage_knn_layers4_lr1e-05.\n",
      "[Epoch 005/15] Tr Loss: 0.2739, Tr F1: 0.3582, Tr AUROC: 0.6248 | Val Loss: 0.2641, Val F1: 0.4240, Val AUROC: 0.6714 | Time: 7.31s\n",
      "  ⭐ New best validation F1: 0.4240 for sage_knn_layers4_lr1e-05.\n",
      "[Epoch 006/15] Tr Loss: 0.2699, Tr F1: 0.3772, Tr AUROC: 0.6484 | Val Loss: 0.2617, Val F1: 0.3914, Val AUROC: 0.6786 | Time: 7.30s\n",
      "[Epoch 007/15] Tr Loss: 0.2667, Tr F1: 0.3782, Tr AUROC: 0.6530 | Val Loss: 0.2612, Val F1: 0.3976, Val AUROC: 0.6851 | Time: 7.31s\n",
      "[Epoch 008/15] Tr Loss: 0.2648, Tr F1: 0.3842, Tr AUROC: 0.6647 | Val Loss: 0.2589, Val F1: 0.4132, Val AUROC: 0.6964 | Time: 7.31s\n",
      "[Epoch 009/15] Tr Loss: 0.2650, Tr F1: 0.3878, Tr AUROC: 0.6664 | Val Loss: 0.2565, Val F1: 0.4393, Val AUROC: 0.7065 | Time: 7.30s\n",
      "  ⭐ New best validation F1: 0.4393 for sage_knn_layers4_lr1e-05.\n",
      "[Epoch 010/15] Tr Loss: 0.2634, Tr F1: 0.3964, Tr AUROC: 0.6729 | Val Loss: 0.2562, Val F1: 0.4281, Val AUROC: 0.7070 | Time: 7.66s\n",
      "[Epoch 011/15] Tr Loss: 0.2640, Tr F1: 0.3927, Tr AUROC: 0.6714 | Val Loss: 0.2560, Val F1: 0.4278, Val AUROC: 0.7076 | Time: 7.31s\n",
      "[Epoch 012/15] Tr Loss: 0.2638, Tr F1: 0.3927, Tr AUROC: 0.6717 | Val Loss: 0.2535, Val F1: 0.4266, Val AUROC: 0.7174 | Time: 7.33s\n",
      "[Epoch 013/15] Tr Loss: 0.2613, Tr F1: 0.4026, Tr AUROC: 0.6807 | Val Loss: 0.2528, Val F1: 0.4276, Val AUROC: 0.7159 | Time: 7.38s\n",
      "[Epoch 014/15] Tr Loss: 0.2614, Tr F1: 0.3998, Tr AUROC: 0.6806 | Val Loss: 0.2520, Val F1: 0.4305, Val AUROC: 0.7167 | Time: 7.33s\n",
      "[Epoch 015/15] Tr Loss: 0.2608, Tr F1: 0.4050, Tr AUROC: 0.6842 | Val Loss: 0.2516, Val F1: 0.4363, Val AUROC: 0.7195 | Time: 7.39s\n",
      "--- ✅ Training Finished for sage_knn_layers4_lr1e-05. Best Val F1: 0.4393 ---\n",
      "Best model for sage_knn_layers4_lr1e-05 saved to: ../EEG_nml/best_model_sage_knn_layers4_lr1e-05.pt\n",
      "\n",
      "--- 🧪 Generating Predictions for sage_knn_layers4_lr1e-05 ---\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='0', CREATED composite_id='pqejgcvm_s001_t000_0'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='1', CREATED composite_id='pqejgcvm_s001_t000_1'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='2', CREATED composite_id='pqejgcvm_s001_t000_2'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='3', CREATED composite_id='pqejgcvm_s001_t000_3'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='4', CREATED composite_id='pqejgcvm_s001_t000_4'\n",
      "Submission CSV for sage_knn_layers4_lr1e-05 saved to: ../EEG_nml/submission_sage_knn_layers4_lr1e-05.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>▂▁▃▅▆▇▇████████</td></tr><tr><td>train_auroc</td><td>▁▂▅▆▆▇▇▇███████</td></tr><tr><td>train_f1</td><td>▁▃▄▆▆▇▇▇▇█▇▇███</td></tr><tr><td>train_loss</td><td>█▃▂▁▁▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>▂▁▅▆▇██████████</td></tr><tr><td>val_auroc</td><td>▁▄▆▇▇▇▇▇███████</td></tr><tr><td>val_f1</td><td>▁▃▄▅▇▆▆▇███████</td></tr><tr><td>val_loss</td><td>█▄▂▂▂▂▂▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>0.61941</td></tr><tr><td>train_auroc</td><td>0.68416</td></tr><tr><td>train_f1</td><td>0.40501</td></tr><tr><td>train_loss</td><td>0.26079</td></tr><tr><td>val_acc</td><td>0.61524</td></tr><tr><td>val_auroc</td><td>0.71948</td></tr><tr><td>val_f1</td><td>0.4363</td></tr><tr><td>val_loss</td><td>0.25162</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">sage_knn_layers4_lr1e-05</strong> at: <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/35jwz6ja' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/35jwz6ja</a><br> View project at: <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project</a><br>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250528_120937-35jwz6ja/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "🏁 COMPLETED EXPERIMENT: sage_knn_layers4_lr1e-05\n",
      "===================================\n",
      "\n",
      "\n",
      "===================================\n",
      "🚀 STARTING EXPERIMENT: gcn_grid_layers4_lr1e-05\n",
      "===================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/my_notebooks/EEG_nml/wandb/run-20250528_121133-gymxdohg</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/gymxdohg' target=\"_blank\">gcn_grid_layers4_lr1e-05</a></strong> to <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/gymxdohg' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/gymxdohg</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB initialized for: gcn_grid_layers4_lr1e-05\n",
      "Loading edge index: ../EEG_nml/edge_index/edge_index_grid.pt\n",
      "Edge index 'grid' loaded. Shape: torch.Size([2, 48])\n",
      "DataLoaders created. Train: 10394, Val: 2599\n",
      "Model (gcn) & Optimizer instantiated.\n",
      "--- 🏋️ Starting Training for: gcn_grid_layers4_lr1e-05 ---\n",
      "[Epoch 001/15] Tr Loss: 0.4342, Tr F1: 0.0641, Tr AUROC: 0.5659 | Val Loss: 0.3711, Val F1: 0.1094, Val AUROC: 0.5652 | Time: 7.46s\n",
      "  ⭐ New best validation F1: 0.1094 for gcn_grid_layers4_lr1e-05.\n",
      "[Epoch 002/15] Tr Loss: 0.3533, Tr F1: 0.1507, Tr AUROC: 0.5883 | Val Loss: 0.3154, Val F1: 0.1712, Val AUROC: 0.5952 | Time: 7.48s\n",
      "  ⭐ New best validation F1: 0.1712 for gcn_grid_layers4_lr1e-05.\n",
      "[Epoch 003/15] Tr Loss: 0.3147, Tr F1: 0.2144, Tr AUROC: 0.6141 | Val Loss: 0.2900, Val F1: 0.3038, Val AUROC: 0.6391 | Time: 7.49s\n",
      "  ⭐ New best validation F1: 0.3038 for gcn_grid_layers4_lr1e-05.\n",
      "[Epoch 004/15] Tr Loss: 0.2932, Tr F1: 0.2825, Tr AUROC: 0.6325 | Val Loss: 0.2801, Val F1: 0.3621, Val AUROC: 0.6512 | Time: 7.48s\n",
      "  ⭐ New best validation F1: 0.3621 for gcn_grid_layers4_lr1e-05.\n",
      "[Epoch 005/15] Tr Loss: 0.2821, Tr F1: 0.3269, Tr AUROC: 0.6431 | Val Loss: 0.2714, Val F1: 0.3795, Val AUROC: 0.6658 | Time: 7.49s\n",
      "  ⭐ New best validation F1: 0.3795 for gcn_grid_layers4_lr1e-05.\n",
      "[Epoch 006/15] Tr Loss: 0.2750, Tr F1: 0.3466, Tr AUROC: 0.6450 | Val Loss: 0.2674, Val F1: 0.3888, Val AUROC: 0.6659 | Time: 7.42s\n",
      "  ⭐ New best validation F1: 0.3888 for gcn_grid_layers4_lr1e-05.\n",
      "[Epoch 007/15] Tr Loss: 0.2733, Tr F1: 0.3531, Tr AUROC: 0.6467 | Val Loss: 0.2644, Val F1: 0.3847, Val AUROC: 0.6765 | Time: 7.44s\n",
      "[Epoch 008/15] Tr Loss: 0.2686, Tr F1: 0.3671, Tr AUROC: 0.6588 | Val Loss: 0.2626, Val F1: 0.3802, Val AUROC: 0.6820 | Time: 7.42s\n",
      "[Epoch 009/15] Tr Loss: 0.2679, Tr F1: 0.3720, Tr AUROC: 0.6566 | Val Loss: 0.2606, Val F1: 0.3877, Val AUROC: 0.6881 | Time: 7.43s\n",
      "[Epoch 010/15] Tr Loss: 0.2659, Tr F1: 0.3767, Tr AUROC: 0.6618 | Val Loss: 0.2606, Val F1: 0.3875, Val AUROC: 0.6846 | Time: 7.45s\n",
      "[Epoch 011/15] Tr Loss: 0.2671, Tr F1: 0.3734, Tr AUROC: 0.6591 | Val Loss: 0.2596, Val F1: 0.4108, Val AUROC: 0.6855 | Time: 7.42s\n",
      "  ⭐ New best validation F1: 0.4108 for gcn_grid_layers4_lr1e-05.\n",
      "[Epoch 012/15] Tr Loss: 0.2644, Tr F1: 0.3833, Tr AUROC: 0.6682 | Val Loss: 0.2587, Val F1: 0.4124, Val AUROC: 0.6969 | Time: 7.75s\n",
      "  ⭐ New best validation F1: 0.4124 for gcn_grid_layers4_lr1e-05.\n",
      "[Epoch 013/15] Tr Loss: 0.2647, Tr F1: 0.3906, Tr AUROC: 0.6664 | Val Loss: 0.2587, Val F1: 0.4123, Val AUROC: 0.6910 | Time: 7.47s\n",
      "[Epoch 014/15] Tr Loss: 0.2652, Tr F1: 0.3898, Tr AUROC: 0.6667 | Val Loss: 0.2580, Val F1: 0.4243, Val AUROC: 0.6954 | Time: 7.44s\n",
      "  ⭐ New best validation F1: 0.4243 for gcn_grid_layers4_lr1e-05.\n",
      "[Epoch 015/15] Tr Loss: 0.2655, Tr F1: 0.3915, Tr AUROC: 0.6647 | Val Loss: 0.2567, Val F1: 0.4260, Val AUROC: 0.7010 | Time: 7.45s\n",
      "  ⭐ New best validation F1: 0.4260 for gcn_grid_layers4_lr1e-05.\n",
      "--- ✅ Training Finished for gcn_grid_layers4_lr1e-05. Best Val F1: 0.4260 ---\n",
      "Best model for gcn_grid_layers4_lr1e-05 saved to: ../EEG_nml/best_model_gcn_grid_layers4_lr1e-05.pt\n",
      "\n",
      "--- 🧪 Generating Predictions for gcn_grid_layers4_lr1e-05 ---\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='0', CREATED composite_id='pqejgcvm_s001_t000_0'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='1', CREATED composite_id='pqejgcvm_s001_t000_1'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='2', CREATED composite_id='pqejgcvm_s001_t000_2'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='3', CREATED composite_id='pqejgcvm_s001_t000_3'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='4', CREATED composite_id='pqejgcvm_s001_t000_4'\n",
      "Submission CSV for gcn_grid_layers4_lr1e-05 saved to: ../EEG_nml/submission_gcn_grid_layers4_lr1e-05.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>█▇▇▆▅▅▄▄▃▃▂▂▁▁▁</td></tr><tr><td>train_auroc</td><td>▁▃▄▆▆▆▇▇▇█▇████</td></tr><tr><td>train_f1</td><td>▁▃▄▆▇▇▇▇███████</td></tr><tr><td>train_loss</td><td>█▅▃▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>██▇▆▆▅▅▄▄▄▃▄▂▁▃</td></tr><tr><td>val_auroc</td><td>▁▃▅▅▆▆▇▇▇▇▇█▇██</td></tr><tr><td>val_f1</td><td>▁▂▅▇▇▇▇▇▇▇█████</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>0.63484</td></tr><tr><td>train_auroc</td><td>0.66475</td></tr><tr><td>train_f1</td><td>0.39151</td></tr><tr><td>train_loss</td><td>0.26552</td></tr><tr><td>val_acc</td><td>0.63601</td></tr><tr><td>val_auroc</td><td>0.70102</td></tr><tr><td>val_f1</td><td>0.42597</td></tr><tr><td>val_loss</td><td>0.2567</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gcn_grid_layers4_lr1e-05</strong> at: <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/gymxdohg' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/gymxdohg</a><br> View project at: <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project</a><br>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250528_121133-gymxdohg/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "🏁 COMPLETED EXPERIMENT: gcn_grid_layers4_lr1e-05\n",
      "===================================\n",
      "\n",
      "\n",
      "===================================\n",
      "🚀 STARTING EXPERIMENT: gcn_knn_layers4_lr1e-05\n",
      "===================================\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.11"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/home/my_notebooks/EEG_nml/wandb/run-20250528_121331-s8tjyyie</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/s8tjyyie' target=\"_blank\">gcn_knn_layers4_lr1e-05</a></strong> to <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/s8tjyyie' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/s8tjyyie</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WandB initialized for: gcn_knn_layers4_lr1e-05\n",
      "Loading edge index: ../EEG_nml/edge_index/edge_index_knn.pt\n",
      "Edge index 'knn' loaded. Shape: torch.Size([2, 152])\n",
      "DataLoaders created. Train: 10394, Val: 2599\n",
      "Model (gcn) & Optimizer instantiated.\n",
      "--- 🏋️ Starting Training for: gcn_knn_layers4_lr1e-05 ---\n",
      "[Epoch 001/15] Tr Loss: 0.4347, Tr F1: 0.0726, Tr AUROC: 0.5660 | Val Loss: 0.3726, Val F1: 0.0998, Val AUROC: 0.5680 | Time: 7.48s\n",
      "  ⭐ New best validation F1: 0.0998 for gcn_knn_layers4_lr1e-05.\n",
      "[Epoch 002/15] Tr Loss: 0.3526, Tr F1: 0.1610, Tr AUROC: 0.5919 | Val Loss: 0.3145, Val F1: 0.1712, Val AUROC: 0.6017 | Time: 7.49s\n",
      "  ⭐ New best validation F1: 0.1712 for gcn_knn_layers4_lr1e-05.\n",
      "[Epoch 003/15] Tr Loss: 0.3151, Tr F1: 0.2241, Tr AUROC: 0.6164 | Val Loss: 0.2893, Val F1: 0.3038, Val AUROC: 0.6436 | Time: 7.47s\n",
      "  ⭐ New best validation F1: 0.3038 for gcn_knn_layers4_lr1e-05.\n",
      "[Epoch 004/15] Tr Loss: 0.2933, Tr F1: 0.2922, Tr AUROC: 0.6355 | Val Loss: 0.2791, Val F1: 0.3621, Val AUROC: 0.6573 | Time: 7.48s\n",
      "  ⭐ New best validation F1: 0.3621 for gcn_knn_layers4_lr1e-05.\n",
      "[Epoch 005/15] Tr Loss: 0.2819, Tr F1: 0.3329, Tr AUROC: 0.6433 | Val Loss: 0.2703, Val F1: 0.3795, Val AUROC: 0.6688 | Time: 7.50s\n",
      "  ⭐ New best validation F1: 0.3795 for gcn_knn_layers4_lr1e-05.\n",
      "[Epoch 006/15] Tr Loss: 0.2750, Tr F1: 0.3431, Tr AUROC: 0.6459 | Val Loss: 0.2662, Val F1: 0.3852, Val AUROC: 0.6705 | Time: 7.48s\n",
      "  ⭐ New best validation F1: 0.3852 for gcn_knn_layers4_lr1e-05.\n",
      "[Epoch 007/15] Tr Loss: 0.2731, Tr F1: 0.3552, Tr AUROC: 0.6488 | Val Loss: 0.2633, Val F1: 0.3847, Val AUROC: 0.6794 | Time: 7.50s\n",
      "[Epoch 008/15] Tr Loss: 0.2683, Tr F1: 0.3661, Tr AUROC: 0.6597 | Val Loss: 0.2613, Val F1: 0.3802, Val AUROC: 0.6843 | Time: 7.47s\n",
      "[Epoch 009/15] Tr Loss: 0.2678, Tr F1: 0.3730, Tr AUROC: 0.6561 | Val Loss: 0.2595, Val F1: 0.4208, Val AUROC: 0.6926 | Time: 7.49s\n",
      "  ⭐ New best validation F1: 0.4208 for gcn_knn_layers4_lr1e-05.\n",
      "[Epoch 010/15] Tr Loss: 0.2654, Tr F1: 0.3815, Tr AUROC: 0.6622 | Val Loss: 0.2594, Val F1: 0.4260, Val AUROC: 0.6902 | Time: 7.48s\n",
      "  ⭐ New best validation F1: 0.4260 for gcn_knn_layers4_lr1e-05.\n",
      "[Epoch 011/15] Tr Loss: 0.2670, Tr F1: 0.3779, Tr AUROC: 0.6584 | Val Loss: 0.2585, Val F1: 0.4149, Val AUROC: 0.6920 | Time: 7.45s\n",
      "[Epoch 012/15] Tr Loss: 0.2640, Tr F1: 0.3864, Tr AUROC: 0.6693 | Val Loss: 0.2574, Val F1: 0.4436, Val AUROC: 0.6989 | Time: 7.46s\n",
      "  ⭐ New best validation F1: 0.4436 for gcn_knn_layers4_lr1e-05.\n",
      "[Epoch 013/15] Tr Loss: 0.2643, Tr F1: 0.3916, Tr AUROC: 0.6675 | Val Loss: 0.2580, Val F1: 0.4188, Val AUROC: 0.6902 | Time: 7.48s\n",
      "[Epoch 014/15] Tr Loss: 0.2650, Tr F1: 0.3931, Tr AUROC: 0.6668 | Val Loss: 0.2570, Val F1: 0.4277, Val AUROC: 0.6959 | Time: 7.47s\n",
      "[Epoch 015/15] Tr Loss: 0.2653, Tr F1: 0.3925, Tr AUROC: 0.6655 | Val Loss: 0.2555, Val F1: 0.4337, Val AUROC: 0.7072 | Time: 7.80s\n",
      "--- ✅ Training Finished for gcn_knn_layers4_lr1e-05. Best Val F1: 0.4436 ---\n",
      "Best model for gcn_knn_layers4_lr1e-05 saved to: ../EEG_nml/best_model_gcn_knn_layers4_lr1e-05.pt\n",
      "\n",
      "--- 🧪 Generating Predictions for gcn_knn_layers4_lr1e-05 ---\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='0', CREATED composite_id='pqejgcvm_s001_t000_0'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='1', CREATED composite_id='pqejgcvm_s001_t000_1'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='2', CREATED composite_id='pqejgcvm_s001_t000_2'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='3', CREATED composite_id='pqejgcvm_s001_t000_3'\n",
      "DEBUG GraphEEGDataset.get() (Test Split): base_id='pqejgcvm_s001_t000', segment_num='4', CREATED composite_id='pqejgcvm_s001_t000_4'\n",
      "Submission CSV for gcn_knn_layers4_lr1e-05 saved to: ../EEG_nml/submission_gcn_knn_layers4_lr1e-05.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>▁▁▂▃▃▃▄▅▅▅▆▇▇▇█</td></tr><tr><td>train_acc</td><td>█▇▇▆▅▅▄▄▃▃▂▂▁▁▁</td></tr><tr><td>train_auroc</td><td>▁▃▄▆▆▆▇▇▇█▇████</td></tr><tr><td>train_f1</td><td>▁▃▄▆▇▇▇▇███████</td></tr><tr><td>train_loss</td><td>█▅▃▂▂▁▁▁▁▁▁▁▁▁▁</td></tr><tr><td>val_acc</td><td>██▇▆▆▅▅▄▄▄▂▃▁▁▂</td></tr><tr><td>val_auroc</td><td>▁▃▅▅▆▆▇▇▇▇▇█▇▇█</td></tr><tr><td>val_f1</td><td>▁▂▅▆▇▇▇▇██▇█▇██</td></tr><tr><td>val_loss</td><td>█▅▃▂▂▂▁▁▁▁▁▁▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>epoch</td><td>15</td></tr><tr><td>train_acc</td><td>0.63281</td></tr><tr><td>train_auroc</td><td>0.66548</td></tr><tr><td>train_f1</td><td>0.39253</td></tr><tr><td>train_loss</td><td>0.26529</td></tr><tr><td>val_acc</td><td>0.64025</td></tr><tr><td>val_auroc</td><td>0.70721</td></tr><tr><td>val_f1</td><td>0.43368</td></tr><tr><td>val_loss</td><td>0.25549</td></tr></table><br/></div></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">gcn_knn_layers4_lr1e-05</strong> at: <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/s8tjyyie' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project/runs/s8tjyyie</a><br> View project at: <a href='https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project' target=\"_blank\">https://wandb.ai/danielebelfiore7-epfl/eeg-gnn-group-project</a><br>Synced 5 W&B file(s), 0 media file(s), 4 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250528_121331-s8tjyyie/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "===================================\n",
      "🏁 COMPLETED EXPERIMENT: gcn_knn_layers4_lr1e-05\n",
      "===================================\n",
      "\n",
      "\n",
      "🎉 All experiment configurations processed! 🎉\n",
      "\n",
      "\n",
      "--- 📊 Experiment Summary Table ---\n",
      "               config_run_id gnn_type edge_variant  num_layers       lr  best_val_f1  final_train_loss  final_train_f1  final_val_loss                                          model_path\n",
      "0   gat_grid_layers4_lr1e-05      gat         grid           4  0.00001     0.469727          0.241210        0.434728        0.233941   ../EEG_nml/best_model_gat_grid_layers4_lr1e-05.pt\n",
      "1    gat_knn_layers4_lr1e-05      gat          knn           4  0.00001     0.490267          0.239774        0.438120        0.232758    ../EEG_nml/best_model_gat_knn_layers4_lr1e-05.pt\n",
      "2  sage_grid_layers4_lr1e-05     sage         grid           4  0.00001     0.441430          0.261336        0.399209        0.252516  ../EEG_nml/best_model_sage_grid_layers4_lr1e-05.pt\n",
      "3   sage_knn_layers4_lr1e-05     sage          knn           4  0.00001     0.439279          0.260789        0.405006        0.251616   ../EEG_nml/best_model_sage_knn_layers4_lr1e-05.pt\n",
      "4   gcn_grid_layers4_lr1e-05      gcn         grid           4  0.00001     0.425971          0.265517        0.391514        0.256699   ../EEG_nml/best_model_gcn_grid_layers4_lr1e-05.pt\n",
      "5    gcn_knn_layers4_lr1e-05      gcn          knn           4  0.00001     0.443590          0.265288        0.392532        0.255490    ../EEG_nml/best_model_gcn_knn_layers4_lr1e-05.pt\n"
     ]
    }
   ],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# ║ 8. TRAIN ALL COMBINATIONS AND SAVE CSVs                              ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import wandb\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch_geometric.loader import DataLoader # Ensure DataLoader is imported\n",
    "\n",
    "# --- ASSUMPTIONS: Ensure these are defined from your notebook before this block ---\n",
    "# PROJECT_BASE_DIR, SCRIPT_PATH, device, set_seed (function)\n",
    "# GraphEEGDataset (class), EEGGNN (class), run_epoch (function)\n",
    "# hf_train, train_idx, val_idx, hf_test (Hugging Face dataset objects and indices)\n",
    "# pos_weight_tensor_for_train (calculated tensor for weighted loss)\n",
    "# global EDGE_INDEX (this will be updated in the loop)\n",
    "# ---------------------------------------------------------------------------------\n",
    "\n",
    "# --- Configurations to iterate through ---\n",
    "gnn_types_to_run = [\"gat\", \"sage\", \"gcn\"]\n",
    "edge_variants_to_run = [\"grid\", \"knn\"] # Ensure 'edge_index_knn.pt' exists in your 'edge_index' folder\n",
    "GAT_HEADS = 4\n",
    "\n",
    "# Example instantiation (tune alpha, gamma, and decide on pos_weight vs alpha)\n",
    "# Option 1: Using alpha (pos_weight inside FocalLoss should be None or 1)\n",
    "# focal_loss_fn = FocalLoss(alpha=0.75, gamma=2.0, reduction='mean') # Assuming class 1 is minority and we weight it by 0.75\n",
    "\n",
    "# Option 2: Using pos_weight directly within FocalLoss's BCE_loss calculation (alpha=None)\n",
    "# Ensure pos_weight_tensor_for_train is calculated and on the correct device\n",
    "focal_loss_fn = FocalLoss(alpha=None, gamma=2.0, reduction='mean', pos_weight=pos_weight_tensor_for_train)\n",
    "\n",
    "# --- Base configuration (parts will be overridden) ---\n",
    "# (Using your provided config, you can adjust epochs for quicker tests initially)\n",
    "base_run_config = {\n",
    "    \"project\": \"eeg-gnn-group-project\",\n",
    "    \"entity\": \"danielebelfiore7-epfl\", # Your WandB entity\n",
    "    \"hidden_dim\": 64,\n",
    "    \"num_layers\": 4,  # Using num_layers from your latest config\n",
    "    \"dropout\": 0.05,\n",
    "    \"lr\": 1e-5,       # Using lr from your latest config\n",
    "    \"weight_decay\": 1e-6,\n",
    "    \"batch_size\": 64,\n",
    "    \"epochs\": 15,     # Using epochs from your latest config\n",
    "    \"seed\": 1,\n",
    "    \"base_dir\": PROJECT_BASE_DIR\n",
    "}\n",
    "\n",
    "# --- To store results for the final table ---\n",
    "experiment_results = []\n",
    "\n",
    "# --- Main Experimental Loop ---\n",
    "for gnn_type_val in gnn_types_to_run:\n",
    "    for edge_variant_val in edge_variants_to_run:\n",
    "        \n",
    "        current_experiment_config = base_run_config.copy()\n",
    "        current_experiment_config[\"gnn_type\"] = gnn_type_val\n",
    "        current_experiment_config[\"edge_variant\"] = edge_variant_val\n",
    "        \n",
    "        run_id_str = f\"{gnn_type_val}_{edge_variant_val}_layers{current_experiment_config['num_layers']}_lr{current_experiment_config['lr']}\"\n",
    "        print(f\"\\n\\n{'='*35}\\n🚀 STARTING EXPERIMENT: {run_id_str}\\n{'='*35}\")\n",
    "\n",
    "        # 1. Initialize WandB\n",
    "        if wandb.run is not None:\n",
    "            wandb.finish() # Ensure any previous run is closed\n",
    "        try:\n",
    "            run = wandb.init(\n",
    "                project=current_experiment_config[\"project\"],\n",
    "                entity=current_experiment_config[\"entity\"],\n",
    "                config=current_experiment_config,\n",
    "                name=run_id_str,\n",
    "                reinit=True,\n",
    "                mode=\"online\" # Set to \"disabled\" if you want to test without logging\n",
    "            )\n",
    "            print(f\"WandB initialized for: {run_id_str}\")\n",
    "        except Exception as e:\n",
    "            print(f\"WandB initialization failed for {run_id_str}: {e}. Running in 'disabled' mode.\")\n",
    "            run = wandb.init(mode=\"disabled\", config=current_experiment_config, name=run_id_str, reinit=True)\n",
    "\n",
    "        # 2. Set Seed\n",
    "        set_seed(current_experiment_config[\"seed\"])\n",
    "\n",
    "        # 3. Load Edge Index (crucially, this needs to be the global EDGE_INDEX if GraphEEGDataset uses it globally)\n",
    "        edge_file_path = os.path.join(current_experiment_config[\"base_dir\"], \"edge_index\", f\"edge_index_{current_experiment_config['edge_variant']}.pt\")\n",
    "        print(f\"Loading edge index: {edge_file_path}\")\n",
    "        try:\n",
    "            EDGE_INDEX = torch.load(edge_file_path, map_location=device).long()\n",
    "            print(f\"Edge index '{current_experiment_config['edge_variant']}' loaded. Shape: {EDGE_INDEX.shape}\")\n",
    "        except FileNotFoundError:\n",
    "            print(f\"🛑 ERROR: Edge index file NOT FOUND at {edge_file_path}. SKIPPING this configuration.\")\n",
    "            if run.mode != \"disabled\": run.finish()\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"🛑 ERROR loading edge index {edge_file_path}: {e}. SKIPPING this configuration.\")\n",
    "            if run.mode != \"disabled\": run.finish()\n",
    "            continue\n",
    "\n",
    "        # 4. Create DataLoaders (these will now use the correct, globally updated EDGE_INDEX when GraphEEGDataset.get is called)\n",
    "        # Assumes hf_train, train_idx, val_idx are already loaded/defined.\n",
    "        iter_train_ds = GraphEEGDataset(hf_train, train_idx)\n",
    "        iter_val_ds = GraphEEGDataset(hf_train, val_idx)\n",
    "        iter_train_loader = DataLoader(iter_train_ds, batch_size=current_experiment_config[\"batch_size\"], shuffle=True, drop_last=len(iter_train_ds) > current_experiment_config[\"batch_size\"])\n",
    "        iter_val_loader = DataLoader(iter_val_ds, batch_size=current_experiment_config[\"batch_size\"], shuffle=False)\n",
    "        print(f\"DataLoaders created. Train: {len(iter_train_ds)}, Val: {len(iter_val_ds)}\")\n",
    "\n",
    "        # 5. Instantiate Model & Optimizer\n",
    "        # Determine input dimension from data if possible, otherwise default.\n",
    "        model_in_dim = iter_train_ds.get(0).x.shape[1] if len(iter_train_ds) > 0 else 9 \n",
    "        \n",
    "        try:\n",
    "            model = EEGGNN(\n",
    "                gnn_type=current_experiment_config[\"gnn_type\"],\n",
    "                in_dim=model_in_dim,\n",
    "                hidden_dim=current_experiment_config[\"hidden_dim\"],\n",
    "                num_layers=current_experiment_config[\"num_layers\"],\n",
    "                dropout=current_experiment_config[\"dropout\"]\n",
    "            ).to(device)\n",
    "\n",
    "            optimizer = torch.optim.Adam(\n",
    "                model.parameters(),\n",
    "                lr=current_experiment_config[\"lr\"],\n",
    "                weight_decay=current_experiment_config[\"weight_decay\"]\n",
    "            )\n",
    "            print(f\"Model ({current_experiment_config['gnn_type']}) & Optimizer instantiated.\")\n",
    "        except Exception as e:\n",
    "            print(f\"🛑 ERROR instantiating model/optimizer for {run_id_str}: {e}. SKIPPING this configuration.\")\n",
    "            if run.mode != \"disabled\": run.finish()\n",
    "            continue\n",
    "            \n",
    "        # 6. Training Loop (adapting your existing loop)\n",
    "        current_best_val_f1 = 0.0\n",
    "        current_best_model_state = None\n",
    "        \n",
    "        print(f\"--- 🏋️ Starting Training for: {run_id_str} ---\")\n",
    "        if run.mode != \"disabled\":\n",
    "            wandb.watch(model, log=\"all\", log_freq=100)\n",
    "\n",
    "        for epoch_num in range(1, current_experiment_config[\"epochs\"] + 1):\n",
    "            epoch_time_start = time.time()\n",
    "            \n",
    "            # IMPORTANT: Ensure your run_epoch function takes model, optimizer, device as args\n",
    "            # e.g., def run_epoch(loader, model, optimizer, device, train_mode=True, is_predict_mode=False, pos_weight_tensor=None):\n",
    "            # def run_epoch(loader, model, train_mode=True, is_predict_mode=False, pos_weight_tensor=None): # Added is_predict_mode and pos_weight_tensor\n",
    "\n",
    "            \n",
    "            metrics_train = run_epoch(iter_train_loader, model, train_mode=True, is_predict_mode=False, pos_weight_tensor=pos_weight_tensor_for_train)\n",
    "            metrics_val = run_epoch(iter_val_loader, model, train_mode=False, is_predict_mode=False, pos_weight_tensor=pos_weight_tensor_for_train)\n",
    "            epoch_time_duration = time.time() - epoch_time_start\n",
    "\n",
    "            if run.mode != \"disabled\":\n",
    "                wandb_metrics_log = {f\"train_{k_met}\": v_met for k_met, v_met in metrics_train.items()}\n",
    "                wandb_metrics_log.update({f\"val_{k_met}\": v_met for k_met, v_met in metrics_val.items()})\n",
    "                wandb_metrics_log[\"epoch\"] = epoch_num\n",
    "                wandb.log(wandb_metrics_log)\n",
    "\n",
    "            print(f\"[Epoch {epoch_num:03d}/{current_experiment_config['epochs']}] \"\n",
    "                  f\"Tr Loss: {metrics_train['loss']:.4f}, Tr F1: {metrics_train['f1']:.4f}, Tr AUROC: {metrics_train['auroc']:.4f} | \"\n",
    "                  f\"Val Loss: {metrics_val['loss']:.4f}, Val F1: {metrics_val['f1']:.4f}, Val AUROC: {metrics_val['auroc']:.4f} | \"\n",
    "                  f\"Time: {epoch_time_duration:.2f}s\")\n",
    "\n",
    "            if metrics_val[\"f1\"] > current_best_val_f1:\n",
    "                current_best_val_f1 = metrics_val[\"f1\"]\n",
    "                current_best_model_state = model.state_dict().copy()\n",
    "                print(f\"  ⭐ New best validation F1: {current_best_val_f1:.4f} for {run_id_str}.\")\n",
    "        \n",
    "        print(f\"--- ✅ Training Finished for {run_id_str}. Best Val F1: {current_best_val_f1:.4f} ---\")\n",
    "\n",
    "        # Save the best model for this specific run\n",
    "        saved_model_path_for_run = None\n",
    "        if current_best_model_state:\n",
    "            best_model_file = f\"best_model_{run_id_str}.pt\"\n",
    "            saved_model_path_for_run = os.path.join(current_experiment_config[\"base_dir\"], best_model_file)\n",
    "            torch.save(current_best_model_state, saved_model_path_for_run)\n",
    "            print(f\"Best model for {run_id_str} saved to: {saved_model_path_for_run}\")\n",
    "            if run.mode != \"disabled\":\n",
    "                artifact_model = wandb.Artifact(f\"{run_id_str}-model\", type=\"model\")\n",
    "                artifact_model.add_file(saved_model_path_for_run)\n",
    "                wandb.log_artifact(artifact_model)\n",
    "        else:\n",
    "            print(f\"No best model state was saved for {run_id_str} (Val F1 did not improve).\")\n",
    "\n",
    "        # Store results for this run (customize what you want to store)\n",
    "        run_summary = {\n",
    "            \"config_run_id\": run_id_str,\n",
    "            \"gnn_type\": current_experiment_config[\"gnn_type\"],\n",
    "            \"edge_variant\": current_experiment_config[\"edge_variant\"],\n",
    "            \"num_layers\": current_experiment_config[\"num_layers\"],\n",
    "            \"lr\": current_experiment_config[\"lr\"],\n",
    "            \"best_val_f1\": current_best_val_f1,\n",
    "            \"final_train_loss\": metrics_train['loss'] if 'metrics_train' in locals() else float('nan'), # Last epoch's train loss\n",
    "            \"final_train_f1\": metrics_train['f1'] if 'metrics_train' in locals() else float('nan'),\n",
    "            \"final_val_loss\": metrics_val['loss'] if 'metrics_val' in locals() else float('nan'), # Last epoch's val loss\n",
    "            \"model_path\": saved_model_path_for_run if saved_model_path_for_run else \"N/A\"\n",
    "        }\n",
    "        experiment_results.append(run_summary)\n",
    "\n",
    "        # 7. Test Set Prediction (Optional - Generate submission CSV for each run)\n",
    "        if saved_model_path_for_run and os.path.exists(saved_model_path_for_run):\n",
    "            print(f\"\\n--- 🧪 Generating Predictions for {run_id_str} ---\")\n",
    "            model.load_state_dict(torch.load(saved_model_path_for_run, map_location=device))\n",
    "            model.eval()\n",
    "\n",
    "            if 'hf_test' not in globals() or hf_test is None: # Load hf_test if not already global\n",
    "                 print(\"Loading hf_test for predictions...\")\n",
    "                 hf_test = load_dataset(path=SCRIPT_PATH, data_dir=current_experiment_config[\"base_dir\"], split=\"test\", name=\"default\")\n",
    "\n",
    "            if len(hf_test) > 0:\n",
    "                iter_test_ds = GraphEEGDataset(hf_test, list(range(len(hf_test)))) # Uses current global EDGE_INDEX\n",
    "                iter_test_loader = DataLoader(iter_test_ds, batch_size=current_experiment_config[\"batch_size\"], shuffle=False)\n",
    "                \n",
    "                # Pass optimizer=None as it's not used in predict_mode\n",
    "                output_test = run_epoch(iter_test_loader, model, train_mode=False, is_predict_mode=True)\n",
    "                \n",
    "                df_submission = pd.DataFrame({\n",
    "                    'id': output_test['signal_ids'],\n",
    "                    'label': output_test['predictions_binary']\n",
    "                })\n",
    "                submission_file = f\"submission_{run_id_str}.csv\"\n",
    "                submission_file_path = os.path.join(current_experiment_config[\"base_dir\"], submission_file)\n",
    "                df_submission.to_csv(submission_file_path, index=False)\n",
    "                print(f\"Submission CSV for {run_id_str} saved to: {submission_file_path}\")\n",
    "                if run.mode != \"disabled\":\n",
    "                    artifact_submission = wandb.Artifact(f\"{run_id_str}-submission\", type=\"predictions\")\n",
    "                    artifact_submission.add_file(submission_file_path)\n",
    "                    wandb.log_artifact(artifact_submission)\n",
    "            else:\n",
    "                print(f\"Skipping prediction for {run_id_str}: hf_test is empty.\")\n",
    "        else:\n",
    "            print(f\"Skipping prediction for {run_id_str}: No best model was saved or found.\")\n",
    "\n",
    "        # 8. Finish WandB run\n",
    "        if run.mode != \"disabled\":\n",
    "            run.finish()\n",
    "        \n",
    "        print(f\"\\n{'='*35}\\n🏁 COMPLETED EXPERIMENT: {run_id_str}\\n{'='*35}\")\n",
    "\n",
    "print(\"\\n\\n🎉 All experiment configurations processed! 🎉\")\n",
    "\n",
    "# --- Display results in a table ---\n",
    "if experiment_results:\n",
    "    results_df = pd.DataFrame(experiment_results)\n",
    "    print(\"\\n\\n--- 📊 Experiment Summary Table ---\")\n",
    "    print(results_df.to_string()) # Print full DataFrame\n",
    "else:\n",
    "    print(\"No results were collected (all runs may have been skipped).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ╔══════════════════════════════════════════════════════════════════════╗\n",
    "# ║ 9. FINALIZE WANDB RUN                                                 ║\n",
    "# ╚══════════════════════════════════════════════════════════════════════╝\n",
    "if wandb.run: # Check if a wandb run is active\n",
    "    wandb.finish()\n",
    "    print(\"WandB run finished.\")\n",
    "else:\n",
    "    print(\"No active WandB run to finish.\")\n",
    "\n",
    "print(\"\\n--- End of Notebook ---\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "2072151e224746a792d7ad2960d0ac3d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "3ab7bfaa6b1f4e19a0b11d5ba8062ede": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "493d65c0b607400f8a95cab5a903dc7d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_cb23647d373a41ada5a4d68c6e027a98",
      "placeholder": "​",
      "style": "IPY_MODEL_e093e37434bf4fb38f8c7fdb319eaaf0",
      "value": " 999/0 [00:03&lt;00:00, 233.95 examples/s]"
     }
    },
    "51cf56ca4d5f489ca8bead2d4c33ad3e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "54d6eeecf4d04cbe88d10de85ca1445a": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9fe842fcd2fb4ed4a174ac383ea747db",
      "placeholder": "​",
      "style": "IPY_MODEL_84fc0f3ec9954ab8a001f0f843ccc5a8",
      "value": "Generating train split: "
     }
    },
    "56ff9c963f9945269d806965b7df1548": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "68983a09643c415cb8fd351faef97383": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_54d6eeecf4d04cbe88d10de85ca1445a",
       "IPY_MODEL_ced5182567074f96b00111c4c9a18985",
       "IPY_MODEL_493d65c0b607400f8a95cab5a903dc7d"
      ],
      "layout": "IPY_MODEL_a56514f486f64509b8ebf56d57ad31bb"
     }
    },
    "6ed01728c0f340bfab91446c373f7774": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "7da68b8fa80a470eb3f5f9d61b9bafc6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "836064a4fbf44e08880d77ac0ea0093e": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_56ff9c963f9945269d806965b7df1548",
      "placeholder": "​",
      "style": "IPY_MODEL_2072151e224746a792d7ad2960d0ac3d",
      "value": " 999/0 [00:09&lt;00:00, 214.27 examples/s]"
     }
    },
    "84fc0f3ec9954ab8a001f0f843ccc5a8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "9fe842fcd2fb4ed4a174ac383ea747db": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "a4626d75a1dc4c56b583423f4218dead": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_d608471227b24cf2ad55f8a1687556f2",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_51cf56ca4d5f489ca8bead2d4c33ad3e",
      "value": 1
     }
    },
    "a56514f486f64509b8ebf56d57ad31bb": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "cb23647d373a41ada5a4d68c6e027a98": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "ced5182567074f96b00111c4c9a18985": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_6ed01728c0f340bfab91446c373f7774",
      "max": 1,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_7da68b8fa80a470eb3f5f9d61b9bafc6",
      "value": 1
     }
    },
    "d36aee82f30f48929b662bac0b11e099": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_e30a199b9fe7407695adb21a7c533cf3",
      "placeholder": "​",
      "style": "IPY_MODEL_e4a9eeefebc64c67b4ba57ccec7a14d9",
      "value": "Generating train split: "
     }
    },
    "d608471227b24cf2ad55f8a1687556f2": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": "20px"
     }
    },
    "dfc1932369d84d048d3ceec60063a5b8": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_d36aee82f30f48929b662bac0b11e099",
       "IPY_MODEL_a4626d75a1dc4c56b583423f4218dead",
       "IPY_MODEL_836064a4fbf44e08880d77ac0ea0093e"
      ],
      "layout": "IPY_MODEL_3ab7bfaa6b1f4e19a0b11d5ba8062ede"
     }
    },
    "e093e37434bf4fb38f8c7fdb319eaaf0": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "e30a199b9fe7407695adb21a7c533cf3": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "e4a9eeefebc64c67b4ba57ccec7a14d9": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
